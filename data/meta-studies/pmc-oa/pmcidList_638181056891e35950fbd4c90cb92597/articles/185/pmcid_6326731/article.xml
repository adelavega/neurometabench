<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" xml:lang="en" dtd-version="1.4"><front><journal-meta><journal-id journal-id-type="nlm-ta">Netw Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Netw Neurosci</journal-id><journal-id journal-id-type="pmc-domain-id">3579</journal-id><journal-id journal-id-type="pmc-domain">netneuro</journal-id><journal-id journal-id-type="publisher-id">netn</journal-id><journal-title-group><journal-title>Network Neuroscience</journal-title></journal-title-group><issn pub-type="epub">2472-1751</issn><publisher><publisher-name>MIT Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC6326731</article-id><article-id pub-id-type="pmcid-ver">PMC6326731.1</article-id><article-id pub-id-type="pmcaid">6326731</article-id><article-id pub-id-type="pmcaiid">6326731</article-id><article-id pub-id-type="pmid">30793072</article-id><article-id pub-id-type="doi">10.1162/netn_a_00050</article-id><article-id pub-id-type="publisher-id">netn_a_00050</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Research Articles</subject></subj-group></article-categories><title-group><article-title>Cooperating yet distinct brain networks engaged during naturalistic paradigms: A meta-analysis of functional MRI results</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-7796-8795</contrib-id><name name-style="western"><surname>Bottenhorn</surname><given-names initials="KL">Katherine L.</given-names></name><xref rid="cor1" ref-type="corresp">*</xref><aff id="aff1">Department of Psychology, Florida International University, Miami, FL, USA</aff></contrib><contrib contrib-type="author"><name name-style="western"><surname>Flannery</surname><given-names initials="JS">Jessica S.</given-names></name><aff id="aff2">Department of Psychology, Florida International University, Miami, FL, USA</aff></contrib><contrib contrib-type="author"><name name-style="western"><surname>Boeving</surname><given-names initials="ER">Emily R.</given-names></name><aff id="aff3">Department of Psychology, Florida International University, Miami, FL, USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-1860-4449</contrib-id><name name-style="western"><surname>Riedel</surname><given-names initials="MC">Michael C.</given-names></name><aff id="aff4">Department of Physics, Florida International University, Miami, FL, USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0001-6363-2759</contrib-id><name name-style="western"><surname>Eickhoff</surname><given-names initials="SB">Simon B.</given-names></name><aff id="aff5">Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University Düsseldorf, Düsseldorf, Germany</aff><aff id="aff6">Institute of Neuroscience and Medicine, Brain &amp; Behaviour (INM-7), Research Centre Jülich, Jülich, Germany</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-6091-4037</contrib-id><name name-style="western"><surname>Sutherland</surname><given-names initials="MT">Matthew T.</given-names></name><aff id="aff7">Department of Psychology, Florida International University, Miami, FL, USA</aff></contrib><contrib contrib-type="author"><name name-style="western"><surname>Laird</surname><given-names initials="AR">Angela R.</given-names></name><aff id="aff8">Department of Physics, Florida International University, Miami, FL, USA</aff></contrib></contrib-group><author-notes><fn fn-type="COI-statement"><p>Competing Interests: The authors have declared that no competing interests exist.</p></fn><corresp id="cor1">* Corresponding Author: <email xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="mailto:alaird@fiu.edu">alaird@fiu.edu</email></corresp><fn><p>Handling Editor: Edward Bullmore</p></fn></author-notes><pub-date pub-type="epub"><day>01</day><month>10</month><year>2018</year><string-date>2018</string-date></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>3</volume><issue>1</issue><issue-id pub-id-type="pmc-issue-id">327243</issue-id><fpage>27</fpage><lpage>48</lpage><history><date date-type="received"><day>21</day><month>7</month><year>2017</year></date><date date-type="accepted"><day>02</day><month>3</month><year>2018</year></date></history><pub-history><event event-type="pmc-release"><date><day>01</day><month>10</month><year>2018</year></date></event><event event-type="pmc-live"><date><day>21</day><month>02</month><year>2019</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2024-07-13 15:25:13.763"><day>13</day><month>07</month><year>2024</year></date></event></pub-history><permissions><copyright-statement>© 2018 Massachusetts Institute of Technology</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Massachusetts Institute of Technology</copyright-holder><license license-type="open-access"><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/legalcode">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="netn-03-27.pdf"/><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="netn-03-27.pdf"/><abstract><p>Cognitive processes do not occur by pure insertion and instead depend on the full complement of co-occurring mental processes, including perceptual and motor functions. As such, there is limited ecological validity to human neuroimaging experiments that use highly controlled tasks to isolate mental processes of interest. However, a growing literature shows how dynamic, interactive tasks have allowed researchers to study cognition as it more naturally occurs. Collective analysis across such neuroimaging experiments may answer broader questions regarding how naturalistic cognition is biologically distributed throughout the brain. We applied an unbiased, data-driven, meta-analytic approach that uses <italic toggle="yes">k</italic>-means clustering to identify core brain networks engaged across the naturalistic functional neuroimaging literature. Functional decoding allowed us to, then, delineate how information is distributed between these networks throughout the execution of dynamical cognition in realistic settings. This analysis revealed six recurrent patterns of brain activation, representing sensory, domain-specific, and attentional neural networks that support the cognitive demands of naturalistic paradigms. Although gaps in the literature remain, these results suggest that naturalistic fMRI paradigms recruit a common set of networks that allow both separate processing of different streams of information and integration of relevant information to enable flexible cognition and complex behavior.</p></abstract><abstract abstract-type="author-summary"><title>Author Summary</title><p>Naturalistic fMRI paradigms offer increased ecological validity over traditional paradigms, addressing the gap left by studying highly interactive cognitive processes as isolated neural phenomena. This study identifies the connectional architecture supporting dynamic cognition in naturalistic fMRI paradigms, the first meta-analysis of a wide range of more realistic neuroimaging experiments. Here we identify and characterize six core patterns of neural activity that support functional segregation and integration in large-scale brain networks. This study provides a unique investigation of the cooperating neural systems that enable complex behavior.</p></abstract><kwd-group kwd-group-type="text"><title>Keywords</title><kwd>Neuroimaging meta-analysis</kwd><x xml:space="preserve">, </x><kwd>Naturalistic paradigms</kwd><x xml:space="preserve">, </x><kwd>Clustering analysis</kwd><x xml:space="preserve">, </x><kwd>Neuroinformatics</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>National Institute on Drug Abuse</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id></institution-wrap></funding-source><award-id>U01-DA041156</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Institute on Drug Abuse</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id></institution-wrap></funding-source><award-id>K01-DA037819</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Institute on Drug Abuse</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id></institution-wrap></funding-source><award-id>U24-DA039832</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Institute on Drug Abuse</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id></institution-wrap></funding-source><award-id>R01DA041353</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Institute of Mental Health</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id></institution-wrap></funding-source><award-id>R56-MH097870</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Science Foundation</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id></institution-wrap></funding-source><award-id>1631325</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Science Foundation</institution><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id></institution-wrap></funding-source><award-id>REAL DRL-1420627</award-id></award-group></funding-group><counts><fig-count count="5"/><table-count count="4"/><ref-count count="79"/><page-count count="22"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-license-ref</meta-name><meta-value>CC BY</meta-value></custom-meta><custom-meta><meta-name>citation</meta-name><meta-value>Bottenhorn, K. L., Flannery, J. S., Boeving, E. R., Riedel, M. C., Eickhoff, S. B., Sutherland, M. T., &amp; Laird, A. R. (2018). Cooperating yet distinct brain networks engaged during naturalistic paradigms: A meta-analysis of functional MRI results. <italic toggle="yes">Network Neuroscience</italic>, <italic toggle="yes">3</italic>(1), 27–48. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://doi.org/10.1162/netn_a_00050">https://doi.org/10.1162/netn_a_00050</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec><title>INTRODUCTION</title><p>Across the life sciences, researchers often seek a balance between ecological validity and careful laboratory control when making experimental design decisions. This entails weighing the value of creating realistic stimuli representative of real-world, interactive experiences versus artificial, reductionist stimuli facilitating precise assessment of “isolated” mental process of interest via cognitive subtraction. <xref rid="def1" ref-type="def">Cognitive subtraction</xref> assumes that a single added cognitive process does not alter the other, co-occurring processes, both neutrally and cognitively. As such, task-based fMRI has traditionally utilized precisely controlled tasks to study the neurobiological substrates of cognition. However, cognition does not occur by pure insertion; the functioning of any cognitive process is not wholly independent from other co-occurring processes (Friston et al., <xref rid="bib26" ref-type="bibr">1996</xref>). Instead, cognition is highly interactive, encompassing measurable changes in neural activity that are dependent on the full amalgamation of relevant social, cognitive, perceptual, and motor processes. Thus, it is perhaps unreasonable to expect findings from a highly restricted assessment of a psychological construct in the scanner to fully generalize to real-world behaviors and settings.</p><p>With advances in technology and a desire to study cognition with greater ecological validity, increasing numbers of studies are utilizing realistic, interactive, and rich stimuli in more ecologically valid experimental designs that fit within the scanner’s confines (Hasson &amp; Honey, <xref rid="bib34" ref-type="bibr">2012</xref>; Maguire, <xref rid="bib47" ref-type="bibr">2012</xref>; Wang et al., <xref rid="bib74" ref-type="bibr">2016</xref>). “Naturalistic” paradigms employ dynamic and complex stimuli (Fehr et al., <xref rid="bib23" ref-type="bibr">2014</xref>; Kauttonen et al., <xref rid="bib38" ref-type="bibr">2015</xref>; Burunat et al., <xref rid="bib11" ref-type="bibr">2014</xref>), in terms of multimodal demands (Lahnakoski et al., <xref rid="bib39" ref-type="bibr">2012</xref>; Maguire, <xref rid="bib47" ref-type="bibr">2012</xref>; Nardo et al., <xref rid="bib53" ref-type="bibr">2014</xref>; Dick et al., <xref rid="bib16" ref-type="bibr">2014</xref>; Reed et al., <xref rid="bib59" ref-type="bibr">2004</xref>; Bishop &amp; Miller, <xref rid="bib6" ref-type="bibr">2009</xref>), or in relation to the length of the stimulus presentation (Maguire, <xref rid="bib47" ref-type="bibr">2012</xref>; Cong et al., <xref rid="bib15" ref-type="bibr">2014</xref>). Specifically, the use of video games, film clips, and virtual reality, among others, has brought a new dimension to cognitive neuroimaging experiments, permitting researchers to study brain activity as participants engage in tasks that more closely represent real-life demands on attention and multimodal sensory integration. Appreciation of such attention and integration processes necessitates more complex stimuli than simple static images presented on a screen. For example, researchers have studied spatial navigation with virtual reality environments as complex as the city of London (Spiers &amp; Maguire, <xref rid="bib63" ref-type="bibr">2006</xref>) and as classic as a virtual radial arm maze (Marsh et al., <xref rid="bib48" ref-type="bibr">2010</xref>). Similarly, social cognition has been probed with displays of human social interactions from a dramatic, social television drama (Spunt &amp; Lieberman, <xref rid="bib65" ref-type="bibr">2012</xref>) to clips of facial expressions with little context (Li et al., <xref rid="bib45" ref-type="bibr">2015</xref>).</p><p>Everyday activities, such as navigation or social observation, involve the integration of processes associated with object recognition, speech comprehension, motor control, and spatial orienting, which all require the interpretation of dynamic signals often from more than one sensory modality (e.g., audiovisual film watching or visuotactile image tracing) and necessitate different attentional demands compared with the simplistic stimuli used in traditional fMRI experiments (Giard &amp; Peronnet, <xref rid="bib28" ref-type="bibr">1999</xref>; McGurk &amp; MacDonald, <xref rid="bib49" ref-type="bibr">1976</xref>; Sailer et al., <xref rid="bib60" ref-type="bibr">2000</xref>; Spence, <xref rid="bib62" ref-type="bibr">2010</xref>). Recently, this trend has produced open-source efforts such as studyforrest, a freely available dataset of MRI scans, eye-tracking, and extensive annotations, using the movie <italic toggle="yes">Forrest Gump</italic> as a rich, multimodal stimulus (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://studyforrest.org">studyforrest.org</ext-link>; Hanke et al., <xref rid="bib31" ref-type="bibr">2016</xref>, <xref rid="bib33" ref-type="bibr">2015</xref>, <xref rid="bib32" ref-type="bibr">2014</xref>). Although studies of participants freely viewing films or navigating virtual environments have been used since the early days of fMRI, the naturalistic studies represent a small portion of the overall task-based fMRI literature (Beauregard et al., <xref rid="bib4" ref-type="bibr">2001</xref>; Burgess et al., <xref rid="bib10" ref-type="bibr">2001</xref>; Maguire, <xref rid="bib47" ref-type="bibr">2012</xref>). Despite offering advantages, the growing body of naturalistic fMRI research has yet to be quantitatively assessed, and little is known of how the neural bases of these tasks support complex information processing and behavioral demands.</p><p>Here, we applied an unbiased, data-driven, meta-analytic approach to quantitatively explore and classify knowledge embedded in the naturalistic fMRI literature. Using an approach developed by Laird et al. (<xref rid="bib41" ref-type="bibr">2015</xref>), we capitalized on the wealth and flexibility of published naturalistic paradigms and investigated recurrent patterns of brain activation reported across a wide variety of tasks and behaviors of interest. This method is based on the premise that functionally similar tasks engage spatially similar patterns of brain activity and that by clustering activation patterns from experimental contrasts, similar experimental paradigms can be identified. Naturalistic paradigms are uniquely rich here because of the multitude of component processes contributing to realistic behavior that can be illuminated by modeling strategies in data analysis. To this end, we extracted relevant information about the stimuli and task demands of these paradigms and assessed motifs in the arrangement of this information, with respect the data-driven clustering analysis, to determine which paradigm aspects elicited activation patterns that subserve common and dissociable cognitive processes. Although naturalistic paradigms vary greatly and are designed to probe a wide range of psychological constructs and behaviors, we hypothesized that complex, multisensory processing are associated with a set of core neural networks engaged by similar content domains and task demands. The objectives of this study were to first elucidate core brain networks engaged by the myriad processes that underlie behavior during naturalistic fMRI paradigms and then to characterize how information processing is potentially distributed between these networks to facilitate complex behaviors in realistic settings.</p></sec><sec><title>METHODS</title><sec><title>Naturalistic fMRI Paradigms</title><p>Here, “naturalistic” paradigms were operationally defined as tasks employing any stimulus that demanded continuous, real-time integration of dynamic streams of information. This definition excludes any paradigms based on still-frame stimulus presentation, which intrinsically impose static constraints that are rarely present in the world and, thus, limit ecologically their validity. Importantly, a key distinction of naturalistic tasks is that stimuli are continuously presented across the duration of the task, whereas other tasks in the literature rely on repeated trials of stimuli. As real-world behavior contextually involves all sensory modalities, we included naturalistic tasks in which such stimuli were presented via the visual, auditory, or tactile modalities or any combination thereof. Visual naturalistic tasks require either a real-time interaction with visual stimuli, in the case of video games and virtual reality, or the continuous integration of real-time information, such as during film viewing. Auditory tasks, including the perception of music and spoken stories, similarly require the continuous integration of, and often interaction with, real-time information. Our operational definition also included tactile naturalistic paradigms, which involve the manipulation and recognition of physical objects. During these tactile tasks, participants gather and integrate sensory information to create a mental representation of the object and, if necessary, form an appropriate behavioral response. Lastly, we note the inclusion of multisensory tasks. As in life, many naturalistic experiments simultaneously present auditory, visual, and tactile information, and such tasks demand the real-time integration of information from multiple sensory modalities.</p></sec><sec><title>Literature Search, Filtering, and Annotation</title><p>An extensive literature search was performed to amass a corpus of naturalistic fMRI studies that were published since the emergence of fMRI in 1992. To identify published naturalistic fMRI studies, PubMed searches were carried out by focusing on stimulus types common to naturalistic research (e.g., video games, film, virtual reality). The first search string, performed on January 13, 2016, used the following string to identify relevant studies by their titles and abstracts: ((“naturalistic”[Title/Abstract] OR “real-world”[Title/Abstract] OR “ecologically valid”[Title/Abstract] OR “true-to-life”[Title/Abstract] OR “realistic”[Title/Abstract] OR “video game”[Title/Abstract] OR “film”[Title/Abstract] OR “movie”[Title/Abstract] OR “virtual reality”[Title/Abstract]) AND (“fMRI”[Title/Abstract] OR “functional magnetic resonance imaging”[Title/Abstract]) AND (“Humans”[MeSH])). This search yielded 679 studies (January 2016), some of which utilized stimulus types that we had not included in our initial query, including music, speech, and tactile objects. To identify any studies using these tasks that may not have been returned by the initial query, a second search was performed on January 20, 2016, using the string ((“music”[Title/Abstract] OR “speech”[Title/Abstract] OR “spoken”[Title/Abstract] OR “tactile object”[Title/Abstract]) AND (“naturalistic”[Title/Abstract] OR “real-world”[Title/Abstract] OR “ecologically valid”[Title/Abstract] OR “true-to-life”[Title/Abstract] OR “realistic”[Title/Abstract]) AND (“fMRI”[Title/Abstract] OR “functional magnetic resonance imaging”[Title/Abstract]) AND “Humans”[MeSH]). This secondary search returned 48 studies, some of which were included in the results of the first search. The two sets of search results were pooled to identify 754 unique studies, which were then reviewed and filtered to identify studies utilizing naturalistic paradigms as defined above.</p><p>Each of 754 candidate studies was first screened and then reviewed according to the following exclusion criteria (<xref ref-type="fig" rid="F1">Figure 1</xref>; Moher, Liberati, Tetzlaff, Altman, &amp; Altman, <xref rid="bib51" ref-type="bibr">2009</xref>). The screening process examined the Abstracts and Methods of each paper to exclude nonnaturalistic tasks in which static, timed blocks of stimuli were presented with a well-defined window for participant response. In this step, we also excluded studies that assessed training or learning across multiple trials or across some period of practice (e.g., pre- vs. post contrasts), as our focus was on neural underpinnings of the tasks themselves and not training-induced changes thereof. In determining eligibility of each paper, studies of participants under the age of 18 or of participants with any history of neurological or psychiatric diagnosis were excluded. After this study-level examination, we then inspected each reported experimental contrast within each paper. In this context, “experiment” represents each statistical parametric image presented, as the result of some functional image data analysis, such as contrasting experimental conditions (Fox et al., <xref rid="bib24" ref-type="bibr">2005</xref>). Experiments from analyses that used an a priori region(s) of interest to investigate activation or functional connectivity were omitted permitting identification of whole-brain neural networks. We also excluded contrasts modeling ANOVA interaction-specific activations because of the inherent complexity of such effects. In this step, any studies/contrasts that did not meet the minimum requirements for coordinate-based meta-analysis, reporting the brain activation locations in a three-dimensional, standardized coordinate space, were discarded.</p><fig id="F1" orientation="portrait" position="float"><label><bold>Figure 1.</bold> </label><caption><p><xref rid="def2" ref-type="def">PRISMA flow chart</xref> of inclusion and exclusion criteria. Each of the experiments returned by the PubMed queries were screened according to this schematic.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="netn-03-27-g001.jpg"/></fig><p>During inspection of each contrast, one study associate (KLB) manually annotated each experiment with terms that described the experimental design with respect to stimulus type utilized, sensory modality engaged, and the task nature. These terms described the salient aspects of the stimuli and behaviors associated with each individual experimental contrast from the corpus of naturalistic paradigms, annotating the particular aspects of the tasks highlighted by each modeled experimental contrast, and not the intended psychological construct interrogated by the original report. These manual annotations were then independently reviewed and confirmed by a second study associate (JSF) to assure consistency and accuracy. Any disagreements or inconsistencies between KLB and JSF were resolved following a final conversation between the two associates.</p></sec><sec><title>Experimental Design and Statistical Analysis</title><sec><title><xref rid="def3" ref-type="def">Modeled activation maps</xref>.</title><p>Following the identification of relevant papers and experiments/contrasts, reported brain activation coordinates were extracted. All Talairach atlas-based coordinates (Talairach &amp; Tournoux, <xref rid="bib68" ref-type="bibr">1988</xref>) were converted to Montreal Neurological Institute (MNI) space (Collins et al., <xref rid="bib14" ref-type="bibr">1994</xref>; Evans et al., <xref rid="bib22" ref-type="bibr">1993</xref>) using the tal2icbm transformation (Lancaster et al., <xref rid="bib43" ref-type="bibr">2007</xref>; Laird et al., <xref rid="bib42" ref-type="bibr">2010</xref>). Probabilistic modeled activation (MA) maps were created from the foci reported in each individual contrast by modeling a spherical Gaussian blur around each focus, with full width at half maximum determined by the number of subjects in each experiment in order to represent the uncertainty induced by the inherent variability from individual differences and between-lab differences (Eickhoff et al., <xref rid="bib19" ref-type="bibr">2009</xref>). These MA maps were concatenated into an array of <italic toggle="yes">n</italic> experiments by <italic toggle="yes">p</italic> voxels, which was then analyzed for pairwise correlations that reflected the degree of spatial similarity between the MA maps from each of the <italic toggle="yes">n</italic> experiments and those of every other experiment. The resultant <italic toggle="yes">n</italic> × <italic toggle="yes">n</italic> correlation matrix represented the similarity of spatial topography of MA maps between every possible pair of experiments.</p></sec><sec><title><xref rid="def4" ref-type="def">K-means clustering</xref> analysis.</title><p>Individual naturalistic experiments (<italic toggle="yes">n</italic> MA maps) were then classified into <italic toggle="yes">K</italic> groups based on their spatial topography similarities. The <italic toggle="yes">k</italic>-means clustering procedure was performed in Matlab (Mathworks, R2013b for Linux), which grouped experiments by pairwise similarity, calculating correlation distance by 1 minus the correlation between MA maps (from the aforementioned correlation matrix) and finding the “best” grouping by minimizing the sum of correlation distances within each cluster (code available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://github.com/62442katieb/meta-analytic-kmeans">https://github.com/62442katieb/meta-analytic-kmeans</ext-link>). This approach begins by choosing <italic toggle="yes">K</italic> arbitrary maps as representative centroids for each of the <italic toggle="yes">K</italic> clusters and assigning experiments to each cluster based on the closest (most similar) centroid. This process continued iteratively until a stable solution was reached.</p><p>Solutions were investigated for a range of <italic toggle="yes">K</italic> = 2–10 clusters. Once the clustering analysis was complete for all <italic toggle="yes">K</italic>, we compared each solution with the neighboring solutions and assessed for improvement across parcellation schemes by using four metrics describing cluster separation and stability (Bzdok et al., <xref rid="bib12" ref-type="bibr">2015</xref>; Eickhoff et al., <xref rid="bib18" ref-type="bibr">2016a</xref>). This allowed us to objectively select the number of clusters that most optimally divided the dataset. The first metric, <italic toggle="yes">average cluster silhouette</italic> across clustering solutions, assessed the separation between clusters and described whether clusters were distinct or overlapping. A higher silhouette value indicates that greater separation is ideal and that each experiment fits well into its cluster, with lower misclassification likelihood of fringe experiments into neighboring clusters. Stability is indicated by a relatively minimal change in silhouette from one solution (<italic toggle="yes">K</italic>) to the next (<italic toggle="yes">K</italic> + 1), indicated by the positive derivative of the silhouette score closest to zero, with greatest stability evidenced by the smallest change between two points. Second, we considered the <italic toggle="yes">consistency of experiment assignment</italic> by comparing the ratio of the minimum number of experiments consistently assigned to a cluster relative to the mean number of experiments consistently assigned to that cluster. In this case, only ratios above 0.5, in which at least half of the experiments were consistently assigned, were considered viable solutions. Third, the <italic toggle="yes">variation of information</italic> was quantified, which compared the entropy of clusters with the mutual information shared between them for each solution <italic toggle="yes">K</italic> and its <italic toggle="yes">K</italic> − 1 and <italic toggle="yes">K</italic> + 1 neighbors. A large decrease in variation of information from <italic toggle="yes">K</italic> − 1 to <italic toggle="yes">K</italic> and increase from <italic toggle="yes">K</italic> to <italic toggle="yes">K</italic> + 1, a local minimum in the plot of variation of information across <italic toggle="yes">K</italic>, indicated a decrease in overlap between solutions and, thus, stability of solution <italic toggle="yes">K</italic>. In this case, “large” is defined, too, in relative terms, with the largest decrease indicating greatest stability of the solutions considered. Finally, we computed a <italic toggle="yes">hierarchy index</italic> for each solution, which assessed how clusters split from the <italic toggle="yes">K</italic> − 1 to <italic toggle="yes">K</italic> solution to form the additional cluster. A lower hierarchy index indicated that clusters present in <italic toggle="yes">K</italic> stemmed from fewer of the clusters present in <italic toggle="yes">K</italic> − 1, another indication of stability in groupings demonstrated by a local minimum across values of <italic toggle="yes">K</italic>. An optimal clustering solution is one that demonstrated minimal overlap between clusters (i.e., high silhouette value), while exhibiting relative stability in comparison with the previous and next solutions (i.e., consistency &gt; 0.5, a local minimum in variation of information, and lower hierarchy index than previous).</p></sec><sec><title>Meta-analytic groupings.</title><p>From the identified optimal clustering solution, we probed the underlying neural topography associated with each of the <italic toggle="yes">K</italic> groups of experiments (Laird et al., <xref rid="bib41" ref-type="bibr">2015</xref>). To this end, the <xref rid="def5" ref-type="def">activation likelihood estimate (ALE)</xref> meta-analysis algorithm (Turkeltaub et al., <xref rid="bib70" ref-type="bibr">2002</xref>; Laird et al., <xref rid="bib40" ref-type="bibr">2005</xref>) was applied to generate a map of convergent activation for each grouping of experiments with similar topography. The ALE algorithm includes a weighting of the number of subjects when computing these maps of convergent activation and accounts for uncertainty associated with individual, template, and registration differences between and across experiments (Eickhoff et al., <xref rid="bib19" ref-type="bibr">2009</xref>; Turkeltaub et al., <xref rid="bib71" ref-type="bibr">2012</xref>). The union of these probability distributions was used to calculate ALE scores, a quantitative assessment of convergence between brain activation across different experiments, which was compared against 1,000 permutations of a null distribution of random spatial arrangements (Eickhoff et al., <xref rid="bib17" ref-type="bibr">2012</xref>). These ALE values for each meta-analytic grouping of experiments were thresholded at <italic toggle="yes">p</italic> &lt; 0.01 (cluster-level corrected for family-wise error) with a voxel-level, cluster-forming threshold of <italic toggle="yes">p</italic> &lt; 0.001 (Eickhoff et al., <xref rid="bib20" ref-type="bibr">2016b</xref>; Woo et al., <xref rid="bib77" ref-type="bibr">2014</xref>). The resultant ALE maps thus reflected the convergent activation patterns within each of the <italic toggle="yes">K</italic> clusters. The experimental <italic toggle="yes">K</italic> clusters are hereafter referred to as <xref rid="def6" ref-type="def">meta-analytic groupings (MAGs)</xref>, representing meta-analytic groups of experiments demonstrating similar activation patterns.</p></sec></sec><sec><title><xref rid="def7" ref-type="def">Functional Decoding</xref></title><p>Once we elucidated convergent activation patterns within MAGs, we sought to gain insight into what aspects of the naturalistic paradigms were most frequently associated with each MAG via functional decoding. Functional decoding is a quantitative, data-driven method by which researchers can infer which mental processes are related to activation in a specific brain region (or set of brain regions) across published fMRI studies. We chose to use two complementary functional decoding approaches, one based on our study-specific, subjective manual annotations mentioned above, and another based on the objective, automated annotations provided by the Neurosynth database for over 11,000 functional neuroimaging studies (Yarkoni et al., <xref rid="bib79" ref-type="bibr">2011</xref>; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://Neurosynth.org">Neurosynth.org</ext-link>). First, the manually annotated terms associated with each experiment were grouped into the MAGs identified above and were assessed by frequency of occurrence in each MAG. The distribution of stimulus modality, stimulus type, and salient terms across MAGs allowed us to evaluate the relationship between activation patterns and the aspects of naturalistic paradigms that elicited them. Second, we included an automated, data-driven annotation method using Neurosynth, which includes automatically extracted terms that occur at a high frequency in the abstract of each archived study. To functionally decode our MAGs, we compared the MAGs’ activation patterns with those reported across published neuroimaging papers in the Neurosynth database. To this end, we uploaded each ALE map to NeuroVault, a web-based repository for 3D statistical neuroimaging maps that directly interfaces with Neurosynth (Gorgolewski et al., <xref rid="bib29" ref-type="bibr">2015</xref>; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://NeuroVault.org">NeuroVault.org</ext-link>). NeuroVault enables “functional decoding” by correlating unthresholded uploaded maps with term-specific meta-analytic maps extracted from Neurosynth’s database of published functional neuroimaging studies. The Neurosynth functional decoding results were exported as a set of terms and correlation values representing how well the spatial distribution of activation associated with each term in the database matched the activation pattern of the uploaded map.</p><p>Both sets of terms (i.e., obtained via manual and automated approaches) were evaluated to assess the specific aspects of naturalistic paradigms associated with each MAG. The Neurosynth terms representing broad behavioral aspects across fMRI studies that elicit similar brain activation profiles provides both an unbiased description of the experiments engaging each MAG, as well as a comparison of our corpus of studies with the broader literature. On the other hand, manual annotation provides a more concise, accurate description of the paradigms, although it is predisposed to the subjective bias of human annotation. The results of this two-pronged functional decoding approach were designed to describe the processes that engage brain networks similar to each MAG and how these processes may be similar or different in naturalistic fMRI studies compared with the broader functional neuroimaging literature. The distribution of stimulus modalities and types across MAGs was assessed, too. Together, the functional decoding results and distributions of different stimuli were interpreted to provide insight into how information processing is functionally segregated across cooperating neural systems during naturalistic tasks.</p></sec></sec><sec><title>RESULTS</title><p>The literature search yielded a combined set of 110 studies that reported coordinates of brain activation from naturalistic fMRI tasks among healthy adults (<xref ref-type="fig" rid="F1">Figure 1</xref>; PubMed IDs available in Supporting Information Table S1, Bottenhorn, Flannery, Boeving, Riedel, Eickhoff, Sutherland, &amp; Laird, <xref rid="bib7" ref-type="bibr">2018</xref>). The final dataset included activation foci from 376 experimental contrasts (<italic toggle="yes">N</italic> = 1,817 subjects) derived from tasks using a variety of stimulus types and sensory modalities. Across our corpus of naturalistic fMRI experiments, approximately 55% assessed a single stimulus modality, including 40% visual stimuli, 13% auditory, and 1% tactile.</p><p>Conversely, 45% of experiments utilized multisensory stimuli, including 41% that employed audiovisual stimuli, 2% in which a visual stimulus was paired with painful, tactile stimuli, and 1% pairing visual and nonpainful tactile stimuli (<xref rid="T1" ref-type="table">Table 1</xref>). Of the visual experiments, 69% involved a motor response, as did 25% of the audiovisual experiments, ranging from a button press to joystick and object manipulation. The stimulus types most frequently used across the included experiments were films (45%), virtual reality (32%), speech (9%), and music (6%) (<xref rid="T2" ref-type="table">Table 2</xref>).</p><table-wrap id="T1" orientation="portrait" position="float"><label><bold>Table 1.</bold> </label><caption><p>Distribution of stimulus modalities across the naturalistic corpus</p></caption><table frame="hsides" rules="groups"><thead><tr valign="bottom"><th align="left" rowspan="1" colspan="1"><bold>Stimulus modality</bold></th><th align="center" rowspan="1" colspan="1"><bold>Number of experiments</bold></th></tr></thead><tbody><tr valign="top"><td align="left" rowspan="1" colspan="1">Auditory</td><td align="center" rowspan="1" colspan="1">50 (13%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Audiovisual</td><td align="center" rowspan="1" colspan="1">154 (41%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual</td><td align="center" rowspan="1" colspan="1">150 (40%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual + tactile (pain)</td><td align="center" rowspan="1" colspan="1">9 (2%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual + tactile</td><td align="center" rowspan="1" colspan="1">5 (1%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Tactile</td><td align="center" rowspan="1" colspan="1">4 (1%)</td></tr></tbody></table><table-wrap-foot><fn id="TBFN1"><p><italic toggle="yes">Note</italic>. Paradigms engaged auditory, visual, and tactile sensory modalities, both separately and in combination.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label><bold>Table 2.</bold> </label><caption><p>Distribution of stimulus types across the naturalistic corpus</p></caption><table frame="hsides" rules="groups"><thead><tr valign="bottom"><th align="left" rowspan="1" colspan="1"><bold>Stimulus type</bold></th><th align="center" rowspan="1" colspan="1"><bold>Number of experiments</bold></th></tr></thead><tbody><tr valign="top"><td align="left" rowspan="1" colspan="1">Film</td><td align="center" rowspan="1" colspan="1">169 (45%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Virtual reality</td><td align="center" rowspan="1" colspan="1">121 (32%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Speech</td><td align="center" rowspan="1" colspan="1">32 (9%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Music</td><td align="center" rowspan="1" colspan="1">21 (6%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Video game</td><td align="center" rowspan="1" colspan="1">13 (4%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">3D image</td><td align="center" rowspan="1" colspan="1">6 (2%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Tactile</td><td align="center" rowspan="1" colspan="1">6 (2%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Picture</td><td align="center" rowspan="1" colspan="1">4 (1%)</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Sounds</td><td align="center" rowspan="1" colspan="1">1 (&lt;1%)</td></tr></tbody></table><table-wrap-foot><fn id="TBFN2"><p><italic toggle="yes">Note</italic>. Within each stimulus modality, multiple types of experimental stimuli were included across the dataset.</p></fn></table-wrap-foot></table-wrap><sec><title><italic toggle="yes">k</italic>-Means Clustering Solutions</title><p>MA maps were created for each contrast and then clustered to identify groups with similar activation topographies. For completeness, the <italic toggle="yes">k</italic>-means clustering solutions for <italic toggle="yes">K</italic> = 2–10 clusters were quantitatively evaluated across four metrics to identify an optimal solution (<xref ref-type="fig" rid="F2">Figure 2</xref>). When considering the average silhouette metric (<xref ref-type="fig" rid="F2">Figure 2A</xref>), values generally increased as <italic toggle="yes">K</italic> increased and the <italic toggle="yes">smallest increase</italic> was observed between <italic toggle="yes">K</italic> = 6 to <italic toggle="yes">K</italic> = 7, indicating little additional separation between clusters gained by moving from 6 to 7 clusters. With respect to the consistency of assigned experiments metric (<xref ref-type="fig" rid="F2">Figure 2B</xref>), each of the solutions <italic toggle="yes">K</italic> = 2–10 met the stability requirement whereby the minimum number of experiments included in any iteration of the solution was at least 50% of the mean number of experiments included across iterations. The variation of information metric (<xref ref-type="fig" rid="F2">Figure 2C</xref>), suggested the stability of a 6-cluster solution as parameter value <italic toggle="yes">decreases</italic> were observed when moving from <italic toggle="yes">K</italic> = 5 to <italic toggle="yes">K</italic> = 6, combined with parameter <italic toggle="yes">increases</italic> when moving from <italic toggle="yes">K</italic> = 6 to <italic toggle="yes">K</italic> = 7, indicating that a 6-cluster solution demonstrates relative stability. The hierarchy index metric (<xref ref-type="fig" rid="F2">Figure 2D</xref>) further corroborated a 6-cluster solution, as a local minimum as observed at <italic toggle="yes">K</italic> = 6. Because of agreement across these metrics, we chose to proceed with the <italic toggle="yes">K</italic> = 6 solution.</p><fig id="F2" orientation="portrait" position="float"><label><bold>Figure 2.</bold> </label><caption><p>Metrics computed for <italic toggle="yes">K</italic> = 2–10 clustering solutions. (A) The average cluster silhouette for each solution <italic toggle="yes">K</italic> from 2 to 10 clusters, showing the distribution of average silhouette values at each value of <italic toggle="yes">K</italic>, resampled 100 times leaving one random experiment out each time. (B) Consistency in experiments assignment to clusters, plotting the minimum consistently assigned clusters next to the mean of consistently assigned clusters. (C) The change in variation of information, a distance metric, from the <italic toggle="yes">K</italic> − 1 to <italic toggle="yes">K</italic> and from <italic toggle="yes">K</italic> to <italic toggle="yes">K</italic> + 1. (D) The hierarchy index for each of <italic toggle="yes">K</italic> clustering solutions, which provides information about how clusters in the <italic toggle="yes">K</italic> solution stemmed from clusters in the <italic toggle="yes">K</italic> − 1 solution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="netn-03-27-g002.jpg"/></fig></sec><sec><title>Meta-Analytic Groupings</title><p>The optimal clustering solution yielded six MAGs of experiments in our corpus, suggesting similarities in brain activation across this sample of the naturalistic literature coalesce into six distinct patterns. The number of experiments that were clustered into each MAG ranged from 50 to 83 experiments (mean = 62.67; <italic toggle="yes">SD</italic> = 12.46). ALE maps of the six MAGs were generated and demonstrated little overlap in activation patterns, suggesting distinct patterns of recurrent activation across our set of naturalistic experiments (<xref ref-type="fig" rid="F3">Figure 3</xref>; Supporting Information Table S2, Bottenhorn et al., <xref rid="bib7" ref-type="bibr">2018</xref>). Whereas some of the MAGs exhibited focal patterns of convergent activation, restricted to a single or neighboring gyri (e.g., MAG 1 and 5), others presented with distributed convergence across multiple lobes (e.g., MAG 2 and 6). Most of the resulting MAGs were restricted to cortical activation patterns, although MAG 3 exhibited convergent activation in subcortical and brainstem regions (results available on NeuroVault at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://neurovault.org/collections/3179/">https://neurovault.org/collections/3179/</ext-link>).</p><fig id="F3" orientation="portrait" position="float"><label><bold>Figure 3.</bold> </label><caption><p>Convergent activation patterns of MAGs from the naturalistic corpus. ALE meta-analysis of experiments in each MAG yielded six patterns of convergent activation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="netn-03-27-g003.jpg"/></fig><p>MAG 1 included convergent activation in the bilateral posterior temporal areas, including portions of the inferior, middle, and superior temporal gyri, extending into the inferior parietal lobule and into the middle occipital gyrus, as well as in the left supramarginal gyrus, right precentral and middle frontal gyri, and in the bilateral precuneus. MAG 2 exhibited convergence in left inferior frontal gyrus, left precentral gyrus, anterior and posterior aspects of the middle temporal gyrus, precuneus, in addition to both the left and right superior frontal gyri. MAG 3 demonstrated a largely symmetric convergence pattern across multiple subcortical structures including bilateral amygdalae, putamen, thalamus, parahippocampal gyrus, and periaqueductal gray, with cortical clusters observed in the left inferior frontal sulcus and inferior frontal gyrus, bilateral anterior cingulate cortex, and bilateral fusiform gyri. MAG 4 exhibited convergent activation in bilateral medial temporal lobes, parahippocampal regions, bilateral precuneus, retrospenial posterior cingulate cortex, occipital regions including the lingual gyrus, right calcarine sulcus, and cuneus, in addition to a small, bilateral portion of the middle frontal gyri. MAG 5 showed convergence in the bilateral superior temporal gyri. MAG 6 demonstrated convergence in the bilateral superior frontal sulci, intraparietal sulci, and superior parietal lobules as well as convergence in higher order visual processing areas in the middle occipital and lingual gyri.</p></sec><sec><title>Stimulus Distribution Across MAGs</title><p>Each stimulus modality was represented in multiple MAGs, but modalities were not evenly distributed across MAGs (<xref ref-type="fig" rid="F4">Figure 4A</xref>). Experiments utilizing audiovisual tasks were somewhat uniformly distributed across the MAGs, with a slightly higher proportion of audiovisual tasks in MAGs 1, 3, and 5. In contrast, more than half of the experiments using auditory tasks were grouped into MAGs 2 and 6. Notably, more experiments based on auditory and audiovisual stimuli were clustered into MAG 5 than any other MAG. Experiments in which participants experienced physical pain were not present in MAGs 1, 5, and 6, but distributed nearly evenly among MAGs 2 through 4, with a slightly higher portion in MAG 3. More than half of experiments that used tactile stimuli were grouped into MAG 5 and 6. Visual experiments were more evenly distributed across clusters, although there was a markedly smaller proportion in MAG 5 than any other MAG. One stimulus type, “sounds,” was represented only once across the corpus and was, thus, excluded from <xref ref-type="fig" rid="F4">Figure 4</xref>. The complete distribution of stimulus modalities across MAGs is provided in Supporting Information Table S3 (Bottenhorn et al., <xref rid="bib7" ref-type="bibr">2018</xref>).</p><fig id="F4" orientation="portrait" position="float"><label><bold>Figure 4.</bold> </label><caption><p>Distribution of stimulus modalities and types across MAGs. (A) The presence of each sensory modality across the corpus that is associated with each MAG. (B) The proportion of each stimulus type present within the corpus that is associated with each MAG. These percentages represent the proportion modality or stimulus type present in each MAG, compared with the total count of that modality or stimulus type across all MAGs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="netn-03-27-g004.jpg"/></fig><p>As with stimulus modality, most stimulus types showed unequal, but not necessarily selective, distribution across MAGs (<xref ref-type="fig" rid="F4">Figure 4B</xref>). Film-based experiments were uniformly distributed across MAGs, and tasks utilizing spoken stimuli were more frequently grouped into MAGs 2 and 5. Again, auditory stimuli were highly associated with MAG 5, as more than 50% of music experiments and 20% of speech experiments were clustered into MAG 5. Experiments that required subjects to play video games were most often grouped into MAGs 4 and 6. Experimental contrasts, which included a condition in which participants received tactile stimulation or manipulated tactile objects, were most prevalent in MAGs 3 and 6. A detailed distribution of stimulus types across MAGs is shown in Supporting Information Table S4 (Bottenhorn et al., <xref rid="bib7" ref-type="bibr">2018</xref>).</p></sec><sec><title>Functional Decoding</title><p>Two approaches for functionally decoding each MAG, manual and automated annotations, were performed to develop a functional interpretation of each MAG’s association with aspects of naturalistic paradigms.</p><sec><title>Manual annotations.</title><p>Our manual annotations utilized a list of 26 corpus-specific metadata terms, which captured salient features of the naturalistic design, rather than the psychological constructs assumed to be involved. <xref rid="T3" ref-type="table">Table 3</xref> displays each of these terms and their frequency of occurrence across MAGs and across the entire corpus (Column = “Total”), highlighting which terms described the largest number of experiments (e.g., “<italic toggle="yes">navigation</italic>,” “<italic toggle="yes">visual features</italic>,” “<italic toggle="yes">emotional film</italic>,” “<italic toggle="yes">attention</italic>”), as well as those that accounted for a minimal number of experiments (e.g., “<italic toggle="yes">violence</italic>,” “<italic toggle="yes">tactile</italic>,” “<italic toggle="yes">pain</italic>”). Values in <xref rid="T3" ref-type="table">Table 3</xref> indicate the percent of experiments labeled with each term, or the base rate of each term throughout the dataset, keeping in mind that each experiment was labeled with only one or two terms. Once the experiments were clustered into six MAGs, we evaluated the relative contributions of each term per MAG, controlling for base rate by dividing each term’s per-MAG count by that term’s total count across the corpus (<xref rid="T3" ref-type="table">Table 3</xref>). We assessed, too, the ability of each term to predict whether an experiment labeled with that term will be clustered into each MAG, (P(MAG|term)) or “forward inference,” and the ability of belongingness to each MAG to predict whether an experiment will be labeled with a particular term, (P(term|MAG)) or “reverse inference.” These outcomes provide the association of each term with each MAG (<xref rid="T3" ref-type="table">Table 3</xref>). Some of the terms in the manual annotation analysis corresponded to stimulus types in <xref ref-type="fig" rid="F4">Figure 4B</xref> (e.g., per-MAG distribution for “<italic toggle="yes">music</italic>” and “<italic toggle="yes">video game</italic>”). However, many of the manually derived terms highlighted experimental aspects that reflect the unique and salient features of the naturalistic corpus (e.g., “<italic toggle="yes">anthropomorphic</italic>,” “<italic toggle="yes">violence</italic>”) and are not included in standard neuroimaging paradigm ontologies such as BrainMap (Fox et al., <xref rid="bib24" ref-type="bibr">2005</xref>) or CogPO (Turner &amp; Laird, <xref rid="bib72" ref-type="bibr">2012</xref>).</p><table-wrap id="T3" orientation="portrait" position="float"><label><bold>Table 3.</bold> </label><caption><p>Manual functional decoding results across meta-analytic groupings</p></caption><table frame="hsides" rules="groups"><thead><tr valign="bottom"><th align="left" rowspan="2" colspan="1"><bold>Term</bold></th><th align="center" colspan="2" rowspan="2"><bold>Total</bold></th><th align="center" colspan="12" rowspan="1"><bold>Frequency per MAG</bold></th></tr><tr valign="bottom"><th align="center" colspan="2" rowspan="1"><bold>MAG 1</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 2</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 3</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 4</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 5</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 6</bold></th></tr></thead><tbody><tr valign="top"><td align="left" rowspan="1" colspan="1">Anthropomorphic</td><td align="center" rowspan="1" colspan="1">21</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">10<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">48%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">19%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">14%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Attention</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">18<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">36%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">6%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">16%</td><td align="center" rowspan="1" colspan="1">10<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">20%</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">18%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Auditory features</td><td align="center" rowspan="1" colspan="1">17</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">6%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">6%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">12%</td><td align="center" rowspan="1" colspan="1">12<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">71%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">6%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Congruence</td><td align="center" rowspan="1" colspan="1">22</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">32%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">18%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">14%</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">27%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Emotional film</td><td align="center" rowspan="1" colspan="1">61</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">17<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">28%</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">17<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">28%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">11<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">18%</td><td align="center" rowspan="1" colspan="1">4<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">7%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Encoding</td><td align="center" rowspan="1" colspan="1">24</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">25%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">13<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">54%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Erotic</td><td align="center" rowspan="1" colspan="1">15</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">8<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">53%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">33%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Faces</td><td align="center" rowspan="1" colspan="1">21</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">24%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">8<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">38%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Imagination</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">26%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">22%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Inference</td><td align="center" rowspan="1" colspan="1">11</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">36%</td><td align="center" rowspan="1" colspan="1">6<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">55%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Language</td><td align="center" rowspan="1" colspan="1">47</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">19%</td><td align="center" rowspan="1" colspan="1">11<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">23%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">6%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">14<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">30%</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">13%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Movement</td><td align="center" rowspan="1" colspan="1">14</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">29%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">14%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">14%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">36%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Music</td><td align="center" rowspan="1" colspan="1">21</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">14%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">14%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">5%</td><td align="center" rowspan="1" colspan="1">11<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">52%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">5%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Narrative</td><td align="center" rowspan="1" colspan="1">30</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">11<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">37%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">13%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Navigation</td><td align="center" rowspan="1" colspan="1">81</td><td align="center" rowspan="1" colspan="1">12%</td><td align="center" rowspan="1" colspan="1">8<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">10<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">12%</td><td align="center" rowspan="1" colspan="1">26<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">32%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">28<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">35%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Negative valence</td><td align="center" rowspan="1" colspan="1">27</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">30%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">11%</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">33%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">15%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">7%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Pain</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">1%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">22%</td><td align="center" rowspan="1" colspan="1">4<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">44%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">33%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Positive valence</td><td align="center" rowspan="1" colspan="1">11</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">18%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">36%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">18%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">18%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Recognition</td><td align="center" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">33%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">8%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">8%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">33%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Retrieval</td><td align="center" rowspan="1" colspan="1">23</td><td align="center" rowspan="1" colspan="1">3%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">22%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">9%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">17%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">43%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Social</td><td align="center" rowspan="1" colspan="1">26</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">35%</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">31%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">8%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">12%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">4%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">12%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Spatial memory</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">1%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">20%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">7<xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">70%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">10%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Tactile</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">1%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">11%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">11%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">33%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">44%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Video game</td><td align="center" rowspan="1" colspan="1">15</td><td align="center" rowspan="1" colspan="1">2%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">27%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">40%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Violence</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">1%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">13%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">25%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">25%</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">25%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">13%</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual features</td><td align="center" rowspan="1" colspan="1">65</td><td align="center" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">23<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">35%</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">6%</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0%</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">15%</td><td align="center" rowspan="1" colspan="1">10<xref ref-type="table-fn" rid="TBFN4">*</xref></td><td align="center" rowspan="1" colspan="1">15%</td><td align="center" rowspan="1" colspan="1">18<xref ref-type="table-fn" rid="TBFN4">*</xref><xref ref-type="table-fn" rid="TBFN5">†</xref></td><td align="center" rowspan="1" colspan="1">28%</td></tr></tbody></table><table-wrap-foot><fn id="TBFN3"><p><italic toggle="yes">Note</italic>. The relative contributions of each manually derived metadata term (e.g., term frequencies) were computed for all MAGs, controlling for the base rate by dividing each term’s per-MAG count by that term’s total count across the corpus. Base rates are provided as the total count for each term.</p></fn><fn id="TBFN4"><label>*</label><p>Significant forward inference at <italic toggle="yes">p</italic><sub>corrected</sub> &lt; 0.05.</p></fn><fn id="TBFN5"><label>†</label><p>Significant reverse inference at <italic toggle="yes">p</italic><sub>corrected</sub> &lt; 0.05 (corrected for false discovery rate).</p></fn></table-wrap-foot></table-wrap></sec><sec><title>Automated Neurosynth annotations.</title><p>To complement the manual annotation analysis, we used Neurosynth’s automated annotations, which describes experiments that engage each MAG based on published neuroimaging data, allowing comparison of our corpus with the broader literature. MAG results were decoded in Neurosynth, yielding correlation values indicating the similarity of the input map (i.e., each MAG’s ALE map) and maps associated with each term from the Neurosynth database. To facilitate interpretation, the top 10 terms with the highest correlation values for each MAG are presented (<xref rid="T4" ref-type="table">Table 4</xref>). Terms that were near-duplicates of terms already included in the list were removed, such as “<italic toggle="yes">emotion</italic>” and “<italic toggle="yes">emotions</italic>” if “<italic toggle="yes">emotional</italic>” was higher on the list. Noncontent terms (e.g., “<italic toggle="yes">abstract</italic>,” “<italic toggle="yes">reliable</italic>”) and terms that described brain regions, such as “<italic toggle="yes">insula</italic>” or “<italic toggle="yes">mt</italic>,” were also excluded.</p><table-wrap id="T4" orientation="portrait" position="float"><label><bold>Table 4.</bold> </label><caption><p>Automated functional decoding results from Neurosynth</p></caption><table frame="hsides" rules="groups"><thead><tr valign="bottom"><th align="center" colspan="2" rowspan="1"><bold>MAG 1</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 2</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 3</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 4</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 5</bold></th><th align="center" colspan="2" rowspan="1"><bold>MAG 6</bold></th></tr><tr valign="bottom"><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th><th align="left" rowspan="1" colspan="1"><bold>NS term</bold></th><th align="center" rowspan="1" colspan="1"><bold>corr.</bold></th></tr></thead><tbody><tr valign="top"><td align="left" rowspan="1" colspan="1">Motion</td><td align="center" rowspan="1" colspan="1">0.555</td><td align="left" rowspan="1" colspan="1">comprehension</td><td align="center" rowspan="1" colspan="1">0.417</td><td align="left" rowspan="1" colspan="1">neutral</td><td align="center" rowspan="1" colspan="1">0.446</td><td align="left" rowspan="1" colspan="1">navigation</td><td align="center" rowspan="1" colspan="1">0.324</td><td align="left" rowspan="1" colspan="1">sounds</td><td align="center" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">visual</td><td align="center" rowspan="1" colspan="1">0.431</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Body</td><td align="center" rowspan="1" colspan="1">0.451</td><td align="left" rowspan="1" colspan="1">sentence</td><td align="center" rowspan="1" colspan="1">0.408</td><td align="left" rowspan="1" colspan="1">fearful</td><td align="center" rowspan="1" colspan="1">0.437</td><td align="left" rowspan="1" colspan="1">Scenes</td><td align="center" rowspan="1" colspan="1">0.316</td><td align="left" rowspan="1" colspan="1">auditory</td><td align="center" rowspan="1" colspan="1">0.732</td><td align="left" rowspan="1" colspan="1">spatial</td><td align="center" rowspan="1" colspan="1">0.414</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Static</td><td align="center" rowspan="1" colspan="1">0.441</td><td align="left" rowspan="1" colspan="1">language</td><td align="center" rowspan="1" colspan="1">0.375</td><td align="left" rowspan="1" colspan="1">facial</td><td align="center" rowspan="1" colspan="1">0.435</td><td align="left" rowspan="1" colspan="1">episodic</td><td align="center" rowspan="1" colspan="1">0.294</td><td align="left" rowspan="1" colspan="1">listening</td><td align="center" rowspan="1" colspan="1">0.711</td><td align="left" rowspan="1" colspan="1">attention</td><td align="center" rowspan="1" colspan="1">0.342</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Moving</td><td align="center" rowspan="1" colspan="1">0.415</td><td align="left" rowspan="1" colspan="1">semantic</td><td align="center" rowspan="1" colspan="1">0.351</td><td align="left" rowspan="1" colspan="1">emotion</td><td align="center" rowspan="1" colspan="1">0.434</td><td align="left" rowspan="1" colspan="1">virtual</td><td align="center" rowspan="1" colspan="1">0.278</td><td align="left" rowspan="1" colspan="1">acoustic</td><td align="center" rowspan="1" colspan="1">0.675</td><td align="left" rowspan="1" colspan="1">eye movements</td><td align="center" rowspan="1" colspan="1">0.300</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Viewed</td><td align="center" rowspan="1" colspan="1">0.406</td><td align="left" rowspan="1" colspan="1">linguistic</td><td align="center" rowspan="1" colspan="1">0.336</td><td align="left" rowspan="1" colspan="1">expressions</td><td align="center" rowspan="1" colspan="1">0.431</td><td align="left" rowspan="1" colspan="1">memory</td><td align="center" rowspan="1" colspan="1">0.276</td><td align="left" rowspan="1" colspan="1">speech</td><td align="center" rowspan="1" colspan="1">0.669</td><td align="left" rowspan="1" colspan="1">execution</td><td align="center" rowspan="1" colspan="1">0.299</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual</td><td align="center" rowspan="1" colspan="1">0.403</td><td align="left" rowspan="1" colspan="1">theory mind</td><td align="center" rowspan="1" colspan="1">0.318</td><td align="left" rowspan="1" colspan="1">happy</td><td align="center" rowspan="1" colspan="1">0.404</td><td align="left" rowspan="1" colspan="1">retrieval</td><td align="center" rowspan="1" colspan="1">0.270</td><td align="left" rowspan="1" colspan="1">music</td><td align="center" rowspan="1" colspan="1">0.625</td><td align="left" rowspan="1" colspan="1">task</td><td align="center" rowspan="1" colspan="1">0.286</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Visual motion</td><td align="center" rowspan="1" colspan="1">0.381</td><td align="left" rowspan="1" colspan="1">mental state</td><td align="center" rowspan="1" colspan="1">0.309</td><td align="left" rowspan="1" colspan="1">angry</td><td align="center" rowspan="1" colspan="1">0.401</td><td align="left" rowspan="1" colspan="1">episodic memory</td><td align="center" rowspan="1" colspan="1">0.258</td><td align="left" rowspan="1" colspan="1">pitch</td><td align="center" rowspan="1" colspan="1">0.612</td><td align="left" rowspan="1" colspan="1">visuospatial</td><td align="center" rowspan="1" colspan="1">0.279</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Videos</td><td align="center" rowspan="1" colspan="1">0.360</td><td align="left" rowspan="1" colspan="1">mind</td><td align="center" rowspan="1" colspan="1">0.306</td><td align="left" rowspan="1" colspan="1">affective</td><td align="center" rowspan="1" colspan="1">0.397</td><td align="left" rowspan="1" colspan="1">place</td><td align="center" rowspan="1" colspan="1">0.208</td><td align="left" rowspan="1" colspan="1">spoken</td><td align="center" rowspan="1" colspan="1">0.590</td><td align="left" rowspan="1" colspan="1">movements</td><td align="center" rowspan="1" colspan="1">0.274</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Perception</td><td align="center" rowspan="1" colspan="1">0.359</td><td align="left" rowspan="1" colspan="1">mentalizing</td><td align="center" rowspan="1" colspan="1">0.304</td><td align="left" rowspan="1" colspan="1">facial expressions</td><td align="center" rowspan="1" colspan="1">0.395</td><td align="left" rowspan="1" colspan="1">autobiographical</td><td align="center" rowspan="1" colspan="1">0.201</td><td align="left" rowspan="1" colspan="1">tones</td><td align="center" rowspan="1" colspan="1">0.572</td><td align="left" rowspan="1" colspan="1">spatial attention</td><td align="center" rowspan="1" colspan="1">0.256</td></tr><tr valign="top"><td align="left" rowspan="1" colspan="1">Observation</td><td align="center" rowspan="1" colspan="1">0.350</td><td align="left" rowspan="1" colspan="1">language comprehension</td><td align="center" rowspan="1" colspan="1">0.289</td><td align="left" rowspan="1" colspan="1">neutral faces</td><td align="center" rowspan="1" colspan="1">0.385</td><td align="left" rowspan="1" colspan="1">remembering</td><td align="center" rowspan="1" colspan="1">0.201</td><td align="left" rowspan="1" colspan="1">voice</td><td align="center" rowspan="1" colspan="1">0.568</td><td align="left" rowspan="1" colspan="1">hand</td><td align="center" rowspan="1" colspan="1">0.250</td></tr></tbody></table><table-wrap-foot><fn id="TBFN6"><p><italic toggle="yes">Note</italic>. The top ten Neurosynth (NS) terms are provided for each MAG, along with the corresponding Pearson’s correlation coefficient (corr.) that indicates the strength of similarity between Neurosynth maps and each MAG.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>Functional interpretation of MAGs.</title><p>The combined knowledge gained from the MAGs topography, as well as the manual and automated metadata decoding analyses, provided insight into the functional interpretation of the clustering results. Overall, the terms yielded by Neurosynth decoding generally agreed with the manual annotation terms in characterizing the MAGs. Below is a summary of the six MAGs; note that reported labels do not refer to the definitive function of these regions, but rather indicate how each MAG reflects differential network contributions during naturalistic fMRI paradigms.</p><p>Manual annotations indicated that MAG 1 experiments involved attention and the processing of dynamic visual features, in addition to visually presented anthropomorphic forms and faces. Most of the stimuli in these experiments were films (<xref ref-type="fig" rid="F4">Figure 4B</xref>), especially affective films. Neurosynth results largely converged with these manual annotations, as terms including “<italic toggle="yes">videos</italic>,” “<italic toggle="yes">body</italic>”, “<italic toggle="yes">observation</italic>,” and “<italic toggle="yes">visual motion</italic>” (<xref rid="T4" ref-type="table">Table 4</xref>) were associated with activations in MAG 1 regions. These annotations, together with the presence of convergent activation across regions commonly associated with higher level visual processing, suggest that MAG 1 was associated with the observation of body and biological motion (<xref ref-type="fig" rid="F3">Figure 3.1</xref>).</p><p>Manual annotations indicated that MAG 2 experiments involved language processing, inference, and judgments about congruence. This MAG included relatively large proportions of the experiments using speech, video games, and tactile stimulation (<xref ref-type="fig" rid="F2">Figure 2B</xref>). Neurosynth results supported the manual annotations’ indication that this MAG was associated with language processing and comprehension, as terms such as “<italic toggle="yes">sentence</italic>,” “<italic toggle="yes">comprehension</italic>,” “<italic toggle="yes">semantic</italic>,” and “<italic toggle="yes">mentalizing</italic>” (<xref rid="T4" ref-type="table">Table 4</xref>) were returned. These annotations and the presence of convergent activation in predominately left lateralized regions typically associated with higher order cognition and language suggest that MAG 2 related to language processing (<xref ref-type="fig" rid="F3">Figure 3.2</xref>).</p><p>Manual annotations indicated that MAG 3 experiments involved human interactions or affective displays, including emotional and erotic films. Films were the predominantly used stimuli across these experiments, while most paradigms using painful stimuli were grouped into this MAG (<xref ref-type="fig" rid="F4">Figure 4B</xref>). Neurosynth results corroborated these manual annotation interpretations regarding affective, aversive, and social processing, with terms such as “<italic toggle="yes">emotion</italic>,” “<italic toggle="yes">facial expressions</italic>,” “<italic toggle="yes">fearful</italic>,” and “<italic toggle="yes">affective</italic>” (<xref rid="T4" ref-type="table">Table 4</xref>). Together, these annotations and a convergent activation pattern involving bilateral amygdalae suggest that MAG 3 was associated with emotional processing (<xref ref-type="fig" rid="F3">Figure 3.3</xref>).</p><p>Manual annotations indicated that MAG 4 heavily represented experiments involving navigation through virtual reality environments, with spatial memory demands related to encoding unfamiliar virtual landscapes for future use. A few of these experiments required language processing as well, and half of the experiments that used 3D images were grouped into MAG 5 (<xref ref-type="fig" rid="F4">Figure 4B</xref>). The manual annotations were reflected in the Neurosynth results, as similar patterns of activation have been associated with “<italic toggle="yes">navigation</italic>,” “<italic toggle="yes">scenes</italic>,” “<italic toggle="yes">memory</italic>,” and “<italic toggle="yes">place</italic>.” Additional related terms added depth to our characterization, expanding on the memory demands with “<italic toggle="yes">retrieval</italic>,” “<italic toggle="yes">episodic memory</italic>,” and “<italic toggle="yes">remembering</italic>” (<xref rid="T4" ref-type="table">Table 4</xref>). Overall, these experimental characteristics and convergent activation in medial temporal regions and along the visual processing stream suggest that MAG 5 was associated with navigation and spatial memory (<xref ref-type="fig" rid="F3">Figure 3.4</xref>).</p><p>Manual annotations showed that MAG 5 experiments primarily involved either film or music stimuli (<xref ref-type="fig" rid="F4">Figure 4B</xref>) and engaged either audiovisual or purely auditory processing (<xref ref-type="fig" rid="F4">Figure 4A</xref>). More than half of the included experiments that used music as stimuli were grouped into this MAG (<xref ref-type="fig" rid="F2">Figure 2B</xref>), with some stimuli involving an emotional quality (<xref rid="T3" ref-type="table">Table 3</xref>). Neurosynth corroborated these interpretations returning terms such as “<italic toggle="yes">auditory</italic>,” “<italic toggle="yes">sounds</italic>,” “<italic toggle="yes">listening</italic>,” and “<italic toggle="yes">speech</italic>” associated with activation of the regions in this MAG. These metadata descriptions combined with convergent activation in superior temporal regions suggest this MAG’s association with auditory processing (<xref ref-type="fig" rid="F3">Figure 3.5</xref>).</p><p>Manual annotations of MAG 6 experiments implicated tasks involving visual attentional demands and the processing of visual features, as participants engaged in video games, tactile stimulation, and virtual reality navigation (<xref ref-type="fig" rid="F4">Figure 4B</xref>, <xref rid="T3" ref-type="table">Table 3</xref>). Stimuli with high visuospatial demands (i.e., video games, virtual reality, and pictures) were represented more by this MAG than any other, whereas stimuli with low visuospatial demands (i.e., music and speech) were represented the least in this MAG. Some experiments involved memory encoding, and visual processing. Neurosynth supported this characterization returning terms including “<italic toggle="yes">visual</italic>,” “<italic toggle="yes">attention</italic>,” “<italic toggle="yes">eye movements</italic>,” “<italic toggle="yes">saccades</italic>,” and “<italic toggle="yes">spatial attention</italic>” associated with activation of the regions in this MAG (<xref rid="T4" ref-type="table">Table 4</xref>). These annotations and convergent activation in regions resembling the dorsal attention network and areas of higher level visual processing (e.g., superior frontal and parietal regions, extrastriate cortex) suggest this MAG’s association with visuospatial attention (<xref ref-type="fig" rid="F3">Figure 3.6</xref>).</p></sec></sec></sec><sec><title>DISCUSSION</title><p>To characterize a core set of brain networks engaged in more ecologically valid neuroimaging designs, we employed a data-driven approach that meta-analytically grouped published naturalistic fMRI results according to their spatial topographies. Objective metrics suggested that a solution of <italic toggle="yes">K</italic> = 6 clusters provided the most stable and disparate grouping of experiments across the naturalistic fMRI literature, and ALE meta-analysis delineated convergent activation across spatially distinct brain regions for each meta-analytic grouping (MAG) of experiments. We then considered how such networks subdivide information processing by assessing the characteristics of the constituent experiments from each MAG. Utilizing both manual and automated functional decoding approaches, enhanced interpretations of the mental processes associated with specific constellations of brain regions were gleaned such that the outcomes of the two approaches generally agreed, with differences highlighting domain-specific and domain-general processes associated with naturalistic paradigms.</p><sec><title>Distributed Processing for Complex Functions</title><p>Although the six identified MAGs are spatially distinct and appear to correspond with dissociable mental processes, most of the included naturalistic tasks that reported more than one statistical contrast recruited more than one MAG (66 of 86). This is consistent with functional segregation and the flexible nature of the naturalistic design, demonstrating that the manipulation of different contrasts can identify distinct networks that likely cooperate to successfully perform a complex task. Further indicative of coordinated interactions and distributed processing, each MAG included experiments that utilized different task modalities and task types. Overwhelmingly, the identified MAGs and the functional characterizations thereof support the notion that complex behaviors are facilitated by coordinated interactions between several large-scale sensory, attentional, and domain-specific networks, a position increasingly endorsed in neuroimaging endeavors (Barrett &amp; Satpute, <xref rid="bib3" ref-type="bibr">2013</xref>; Lindquist et al., <xref rid="bib46" ref-type="bibr">2012</xref>; Mišić &amp; Sporns, <xref rid="bib50" ref-type="bibr">2016</xref>; Spreng et al., <xref rid="bib64" ref-type="bibr">2013</xref>). The characterization of identified MAGs from aspects of the naturalistic paradigms that elicit them suggest an information processing model of cooperating systems (<xref ref-type="fig" rid="F5">Figure 5</xref>) for sensory input (MAGs 1 and 5), attentional control (MAG 6), and domain-specific processing (MAGs 2, 3, and 4), into and from which information is segregated and integrated to enable complex behaviors (e.g., language, emotion, spatial navigation). Output relevant to the corresponding input would be relegated by motor planning and execution systems, which are notably absent from the characterization of MAGs presented here, as experiments requiring a motor response were evenly distributed across MAGs, rather than clustered together.</p><fig id="F5" orientation="portrait" position="float"><label><bold>Figure 5.</bold> </label><caption><p>Complex systems for dynamical information processing. The identified MAGs present a framework of component systems that interact to enable complex information processing needed for naturalistic behavior, including necessary input systems, as well as systems for modality-specific (indicated by dashed line) visuospatial attentional gating of irrelevant information and domain-specific processing for language-, emotion-, and navigation-related tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="netn-03-27-g005.jpg"/></fig><p>MAGs 1 and 5 primarily represent the perceptual processing streams of incoming auditory and visual information, and likely cooperate to process audiovisual information. Functional decoding suggests that MAG 1 is involved in viewing faces and anthropomorphic figures, which is consistent with previous research showing that posterior temporal and temporo-occipital regions corresponding with area V5/MT are associated with the perception of movement, specifically biological movement (Cohen Kadosh et al., <xref rid="bib13" ref-type="bibr">2010</xref>; Pelphrey et al., <xref rid="bib55" ref-type="bibr">2005</xref>, <xref rid="bib54" ref-type="bibr">2004</xref>; Puce et al., <xref rid="bib57" ref-type="bibr">1998</xref>; Wheaton et al., <xref rid="bib75" ref-type="bibr">2004</xref>). Similarly, MAG 5 is associated with listening to music and speech, as well as perceiving pitch and tone, stretching across primary auditory cortex and into regions of higher auditory processing (Gray et al., <xref rid="bib30" ref-type="bibr">2009</xref>; Türe et al., <xref rid="bib69" ref-type="bibr">1999</xref>). Per functional decoding of MAG 6 of both manual and automated annotations, MAG 6 is associated with visuospatial attention. This functional characterization is also supported by corresponding fronto-parietal activations that are often associated with attending to visual stimuli (Braga et al., <xref rid="bib8" ref-type="bibr">2016</xref>; Puschmann et al., <xref rid="bib58" ref-type="bibr">2016</xref>). MAGs 1 and 5 represent the perceptual processing streams of audiovisual information.</p><p>Information processing depends on input from perceptual systems, filtered by attentional gating, but proceeds in a functionally segregated manner, seen in domain-specific MAGs for linguistic, emotional, and spatial processing. When considering language processing, there is necessary input to primary auditory areas (MAG 5) that is further processed by higher level language areas that facilitate speech perception and comprehension (MAG 2). More than a third of contrasts from experiments that utilized speech-based paradigms contributed to the convergent activation pattern of MAG 2, which was linked by both functional decoding techniques to language-related processes. Furthermore, the regions of MAG 2 resemble a neural “language network” (Friederici &amp; Gierhan, <xref rid="bib25" ref-type="bibr">2013</xref>; Heim et al., <xref rid="bib35" ref-type="bibr">2003</xref>; Price, <xref rid="bib56" ref-type="bibr">2010</xref>; Saur et al., <xref rid="bib61" ref-type="bibr">2010</xref>), including some regions associated with orofacial articulation (lip, tongue, and jaw movements) and motor planning (SMA, pre-SMA) that allow the motor components of speech. By presenting language in a context that is more representative of how we process language in everyday life, such as through the use of spoken fictional narratives (AbdulSabur et al., <xref rid="bib1" ref-type="bibr">2014</xref>; Wallentin et al., <xref rid="bib73" ref-type="bibr">2011</xref>; Xu et al., <xref rid="bib78" ref-type="bibr">2005</xref>) or scene descriptions (Summerfield et al., <xref rid="bib67" ref-type="bibr">2010</xref>), naturalistic fMRI paradigms allow researchers to explore the multiple neural networks at work in performing the cooperating processes that facilitate language processing. Similarly, emotional processing (MAG 3) often necessitates audiovisual input (MAGs 1 and 5) and necessitates attention (MAG 6). Emotional films recruited regions across these four MAGs, suggesting a similarly diverse group of coordinated neural systems are engaged when observing affective displays. Additionally, navigation (Burgess et al., <xref rid="bib9" ref-type="bibr">2002</xref>; Kalpouzos et al., <xref rid="bib36" ref-type="bibr">2010</xref>; Wolbers et al., <xref rid="bib76" ref-type="bibr">2004</xref>) depends on visual input (MAG 1), effective visuospatial attentional (MAG 6), and spatial memory and processing (MAG 4). The functional characterization of MAG 4 from manual and Neurosynth decoding highlights its involvement in navigation and spatial memory, supported by studies of rats and humans with brain lesions that indicate the importance of medial temporal, hippocampal, and precuneus regions in processing visual scenes and spatial information (Bird &amp; Burgess, <xref rid="bib5" ref-type="bibr">2008</xref>; Epstein, <xref rid="bib21" ref-type="bibr">2008</xref>; Lee et al., <xref rid="bib44" ref-type="bibr">2005</xref>; Sailer et al., <xref rid="bib60" ref-type="bibr">2000</xref>; Squire et al., <xref rid="bib66" ref-type="bibr">2004</xref>; Summerfield et al., <xref rid="bib67" ref-type="bibr">2010</xref>; Xu et al., <xref rid="bib78" ref-type="bibr">2005</xref>).</p><p>Finally, the characterization of MAG 6 indicates a domain-specific attentional system, as both manual and automated Neurosynth decoding highlight its involvement in visual processing in the absence of any association with other modalities. This is reflected by the distributions of stimuli across MAGs (<xref ref-type="fig" rid="F4">Figure 4</xref>), which show low numbers of auditory and pain-related stimuli represented in MAG 6, while rich visual stimuli that include spatial information (i.e., video games, virtual reality, and pictures) are highly represented across the experiments in MAG 6. Curiously, tactile object manipulation was highly represented in MAG 6, representing the perception of spatial information in the absence of visual information (<xref ref-type="fig" rid="F4">Figure 4</xref>, <xref rid="T3" ref-type="table">Table 3</xref>). Together, these suggest that MAG 6 provides modality-specific attentional gating, depicted by the dashed line in <xref ref-type="fig" rid="F5">Figure 5</xref>.</p></sec><sec><title>Limitations</title><p>The present results may be limited by the <italic toggle="yes">k</italic>-means clustering method, which is limited by the assumptions of the algorithm and underlying topology of the data, as it is sensitive to spherical clusters and assumes the data are linearly separable. Furthermore, there is a potential for bias with this method; certain parameters are specified by the researcher beforehand. To address this potential for bias and the stability of our clustering solution, we performed duplicate clustering analyses with both linear (hierarchical clustering using Ward’s method) and nonlinear (kernel <italic toggle="yes">k</italic>-means and density-based spatial clustering) methods. The results of these analyses are provided in the Supporting information (Figures S1–S3, Bottenhorn et al., <xref rid="bib7" ref-type="bibr">2018</xref>) and confirmed that our choice of the <italic toggle="yes">k</italic>-means clustering method provided optimal separation of the data into six clusters. Experiments in our corpus were grouped using the <italic toggle="yes">kmeans++</italic> algorithm for each of <italic toggle="yes">K</italic> = 2 through <italic toggle="yes">K</italic> = 20 solutions, repeated 1,000 times to ensure that each solution minimized the point-to-centroid distance, indicative of optimal clustering (Kanungo et al., <xref rid="bib37" ref-type="bibr">2004</xref>). Pearson’s correlation was selected as the distance metric, as recommended by Laird et al. (<xref rid="bib41" ref-type="bibr">2015</xref>). The <italic toggle="yes">K</italic> = 6 solution was designated as an optimal candidate solution before assessing the convergent activation patterns of each MAG, based on the aforementioned metrics, yielding a data-driven result. These results are, of course, influenced by the choice of clustering method, and should be considered accordingly. As this was a meta-analytic effort, it is limited, too, by the initial modeling of the data. Despite this, coordinate-based meta-analyses are considered a robust method for synthesis of previously published functional neuroimaging literature (Eickhoff et al., <xref rid="bib17" ref-type="bibr">2012</xref>, <xref rid="bib19" ref-type="bibr">2009</xref>; Fox et al., <xref rid="bib24" ref-type="bibr">2005</xref>). Although the functional decoding-based manual annotations relied on a subjective process, the results were largely confirmed by comparison with the wider body of functional neuroimaging literature facilitated by Neurosynth’s automated functional decoding. It is worth noting that the naturalistic literature is somewhat limited, with an emphasis on navigation and affective processing, and continued research and expansion of this corpus will facilitate development of a more comprehensive model of the neural networks that support realistic behavior.</p></sec><sec><title>Summary and Future Work</title><p>In summary, this meta-analysis of naturalistic fMRI studies that apply dynamic, lifelike tasks to explore the neural correlates of behavior has shown that these paradigms engage a set of core neural networks, supporting both separate processing of different streams of information and the integration of related information to enable flexible cognition and complex behavior. We identified seven patterns of consistent activation that correspond with neural networks that are involved in sensory input, top-down attentional control, domain-specific processing, and motor planning, representing the set of behavioral processes elicited by naturalistic paradigms in our corpus. Across the corpus, tasks provided mainly visual and auditory sensory input that engaged regions across MAGs 1 and 5, while MAG 6 appeared to contribute to top-down attentional control to filter out nonessential visual and/or spatial information. Salient information can be processed by the relevant domain-specific networks, shown in MAGs 2 (language), 3 (emotion), and 4 (navigation and spatial memory), informing the appropriate response. Most naturalistic tasks engaged multiple networks to process the relevant information from a stimulus and generate an appropriate response. A shift in favor of utilizing naturalistic paradigms, when possible, would greatly benefit the field, as naturalistic stimuli more closely approximate the full complement of processing necessary for realistic behavior. Because of the availability of naturalistic fMRI data from sources such as studyforrest.org, the Human Connectome Project, and the Healthy Brain Network Serial Scanning Initiative (HBNSSI), an intriguing next step in this line of work would include validating these MAGs in the primary analysis of imaging data. Exploring how multifaceted processes interact and, ultimately, contribute to behavior will allow us to better understand the brain and human behavior in the real world. In the future, studies of this sort would greatly benefit from an automated annotation process for an objective functional decoding of included papers, instead of subjective manual annotation.</p></sec></sec><sec><title>SUPPORTING INFORMATION</title><p>The authors have released all code and data associated with this manuscript. The code and tabular data are available on GitHub (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://github.com/62442katieb/meta-analytic-kmeans">https://github.com/62442katieb/meta-analytic-kmeans</ext-link>), and the unthresholded maps of each MAG are available on NeuroVault (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://neurovault.org/collections/3179/">https://neurovault.org/collections/3179/</ext-link>). Supporting Information including tables and figures cited above is available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://doi.org/10.1162/netn_a_00050">https://doi.org/10.1162/netn_a_00050</ext-link>.</p></sec><sec><title>AUTHOR CONTRIBUTIONS</title><p>Katherine L. Bottenhorn: Formal analysis; Investigation; Methodology; Writing – original draft. Jessica S. Flannery: Formal analysis; Writing – review &amp; editing. Emily R. Boeving: Writing – review &amp; editing. Michael C. Riedel: Formal analysis; Methodology; Writing – review &amp; editing. Simon B. Eickhoff: Resources; Software; Writing – review &amp; editing. Matthew T. Sutherland: Conceptualization; Resources; Writing – review &amp; editing. Angela R. Laird: Conceptualization; Funding acquisition; Project administration; Resources; Software; Writing – review &amp; editing.</p></sec><sec><title>FUNDING INFORMATION</title><p>Angela R Laird, National Institute on Drug Abuse (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000026">http://dx.doi.org/10.13039/100000026</ext-link>), Award ID: U01-DA041156. Matthew T Sutherland, National Institute on Drug Abuse (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000026">http://dx.doi.org/10.13039/100000026</ext-link>), Award ID: K01-DA037819. Not Applicable, National Institute on Drug Abuse (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000026">http://dx.doi.org/10.13039/100000026</ext-link>), Award ID: U24-DA039832. Not Applicable, National Institute on Drug Abuse (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000026">http://dx.doi.org/10.13039/100000026</ext-link>), Award ID: R01DA041353. Angela R Laird, National Institute of Mental Health (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000025">http://dx.doi.org/10.13039/100000025</ext-link>), Award ID: R56-MH097870. Angela R Laird, National Science Foundation (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000001">http://dx.doi.org/10.13039/100000001</ext-link>), Award ID: 1631325. Angela R Laird, National Science Foundation (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000001">http://dx.doi.org/10.13039/100000001</ext-link>), Award ID: REAL DRL-1420627.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SMS1" position="float" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="netn-03-27-s001.pdf" position="float" orientation="portrait"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><glossary><title>TECHNICAL TERMS</title><def-list><def-item id="def1"><term>Cognitive subtraction:</term><def><p>Method of isolating specific aspects of cognition that assumes cognitive processes do not interact when they co-occur.</p></def></def-item><def-item id="def2"><term>PRISMA flow chart:</term><def><p>Describes how papers are systematically reviewed for inclusion, specified by the Preferred Reporting Items for Systematic Review and Meta-Analysis.</p></def></def-item><def-item id="def3"><term>Modeled activation (MA) maps:</term><def><p>Whole-brain maps recreating results from published neuroimaging studies as Gaussian kernels centered at each reported <italic toggle="yes">x</italic>-, <italic toggle="yes">y</italic>-, and <italic toggle="yes">z</italic>-coordinate of activation.</p></def></def-item><def-item id="def4"><term><italic toggle="yes">K</italic>-means clustering:</term><def><p>Method by which <italic toggle="yes">n</italic> experiments are assigned to one of <italic toggle="yes">k</italic> clusters, based on proximity to the cluster’s center.</p></def></def-item><def-item id="def5"><term>Activation likelihood estimate (ALE):</term><def><p>Process by which the voxel-wise union of MA maps is calculated to create a meta-analytic, statistical map of brain activation.</p></def></def-item><def-item id="def6"><term>Meta-analytic groupings (MAGs):</term><def><p>Each of <italic toggle="yes">k</italic> clusters of experiments, cluster assignment via <italic toggle="yes">k</italic>-means clustering.</p></def></def-item><def-item id="def7"><term>Functional decoding:</term><def><p>The use of previous neuroimaging results to understand the probability of some mental process, given a pattern of brain activation.</p></def></def-item></def-list></glossary><ref-list><title>REFERENCES</title><ref id="bib1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>AbdulSabur</surname><given-names>N. Y.</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Chow</surname><given-names>H. M.</given-names></name>, <name name-style="western"><surname>Baxter</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Carson</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Braun</surname><given-names>A. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Neural correlates and network connectivity underlying narrative production and comprehension: A combined fMRI and PET study</article-title>. <source>Cortex</source>, <volume>57</volume>, <fpage>107</fpage>–<lpage>127</lpage>. <pub-id pub-id-type="pmid">24845161</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cortex.2014.01.017</pub-id></mixed-citation></ref><ref id="bib2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Apps</surname><given-names>M. A. J.</given-names></name>, <name name-style="western"><surname>Lockwood</surname><given-names>P. L.</given-names></name>, &amp; <name name-style="western"><surname>Balsters</surname><given-names>J. H.</given-names></name></person-group> (<year>2013</year>). <article-title>The role of the midcingulate cortex in monitoring others’ decisions</article-title>. <source>Frontiers in Neuroscience</source>, <volume>7</volume>, <fpage>251</fpage><pub-id pub-id-type="pmid">24391534</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fnins.2013.00251</pub-id><pub-id pub-id-type="pmcid">PMC3868891</pub-id></mixed-citation></ref><ref id="bib3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barrett</surname><given-names>L. F.</given-names></name>, &amp; <name name-style="western"><surname>Satpute</surname><given-names>A. B.</given-names></name></person-group> (<year>2013</year>). <article-title>Large-scale brain networks in affective and social neuroscience: Towards an integrative functional architecture of the brain</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume>, <fpage>361</fpage>–<lpage>372</lpage>. <pub-id pub-id-type="pmid">23352202</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.conb.2012.12.012</pub-id><pub-id pub-id-type="pmcid">PMC4119963</pub-id></mixed-citation></ref><ref id="bib4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beauregard</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lévesque</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Bourgouin</surname><given-names>P.</given-names></name></person-group> (<year>2001</year>). <article-title>Neural correlates of conscious self-regulation of emotion</article-title>. <source>Journal of Neuroscience</source>, <volume>21</volume>, <fpage>RC165</fpage>.<pub-id pub-id-type="pmid">11549754</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1523/JNEUROSCI.21-18-j0001.2001</pub-id><pub-id pub-id-type="pmcid">PMC6763007</pub-id></mixed-citation></ref><ref id="bib5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bird</surname><given-names>C. M.</given-names></name>, &amp; <name name-style="western"><surname>Burgess</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <article-title>The hippocampus and memory: Insights from spatial processing</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>9</volume>, <fpage>182</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="pmid">18270514</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/nrn2335</pub-id></mixed-citation></ref><ref id="bib6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>C. W.</given-names></name>, &amp; <name name-style="western"><surname>Miller</surname><given-names>L. M.</given-names></name></person-group> (<year>2009</year>). <article-title>A multisensory cortical network for understanding speech in noise</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>21</volume>, <fpage>1790</fpage>–<lpage>1805</lpage>. <comment>Available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://www.mitpressjournals.org/doi/10.1162/jocn.2009.21118">http://www.mitpressjournals.org/doi/10.1162/jocn.2009.21118</ext-link>, [Accessed October 26, 2017]</comment>.<pub-id pub-id-type="pmid">18823249</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/jocn.2009.21118</pub-id><pub-id pub-id-type="pmcid">PMC2833290</pub-id></mixed-citation></ref><ref id="bib7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bottenhorn</surname><given-names>K. L.</given-names></name>, <name name-style="western"><surname>Flannery</surname><given-names>J. S.</given-names></name>, <name name-style="western"><surname>Boeving</surname><given-names>E. R.</given-names></name>, <name name-style="western"><surname>Riedel</surname><given-names>M. C.</given-names></name>, <name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Sutherland</surname><given-names>M. T.</given-names></name>, &amp; <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name></person-group> (<year>2018</year>). <article-title>Supporting Information for “Cooperating yet distinct brain networks engaged during naturalistic paradigms: A meta-analysis of functional MRI results.”</article-title><source>Network Neuroscience</source>, <volume>3</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/netn_a_00050</pub-id><pub-id pub-id-type="pmcid">PMC6326731</pub-id><pub-id pub-id-type="pmid">30793072</pub-id></mixed-citation></ref><ref id="bib8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Braga</surname><given-names>R. M.</given-names></name>, <name name-style="western"><surname>Fu</surname><given-names>R. Z.</given-names></name>, <name name-style="western"><surname>Seemungal</surname><given-names>B. M.</given-names></name>, <name name-style="western"><surname>Wise</surname><given-names>R. J. S.</given-names></name>, &amp; <name name-style="western"><surname>Leech</surname><given-names>R.</given-names></name></person-group> (<year>2016</year>). <article-title>Eye movements during auditory attention predict individual differences in dorsal attention network activity</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>10</volume>, <fpage>164</fpage><pub-id pub-id-type="pmid">27242465</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fnhum.2016.00164</pub-id><pub-id pub-id-type="pmcid">PMC4860869</pub-id></mixed-citation></ref><ref id="bib9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burgess</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Maguire</surname><given-names>E. A.</given-names></name>, &amp; <name name-style="western"><surname>O’Keefe</surname><given-names>J.</given-names></name></person-group> (<year>2002</year>). <article-title>The human hippocampus and spatial and episodic memory</article-title>. <source>Neuron</source>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/s0896-6273(02)00830-9</pub-id><pub-id pub-id-type="pmid">12194864</pub-id></mixed-citation></ref><ref id="bib10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burgess</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Maguire</surname><given-names>E. A.</given-names></name>, <name name-style="western"><surname>Spiers</surname><given-names>H. J.</given-names></name>, &amp; <name name-style="western"><surname>O’Keefe</surname><given-names>J.</given-names></name></person-group> (<year>2001</year>). <article-title>A temporoparietal and prefrontal network for retrieving the spatial context of lifelike events</article-title>. <source>Neuroimage</source>, <volume>14</volume>, <fpage>439</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="pmid">11467917</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/nimg.2001.0806</pub-id></mixed-citation></ref><ref id="bib11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burunat</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Alluri</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Toiviainen</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Numminen</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Brattico</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>Dynamics of brain activity underlying working memory for music in a naturalistic condition</article-title>. <source>Cortex</source>, <volume>57</volume>, <fpage>254</fpage>–<lpage>269</lpage>.<pub-id pub-id-type="pmid">24949579</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cortex.2014.04.012</pub-id></mixed-citation></ref><ref id="bib12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bzdok</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Heeger</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Langner</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name>, <name name-style="western"><surname>Palomero-Gallagher</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Vogt</surname><given-names>B. A.</given-names></name>, <name name-style="western"><surname>Zilles</surname><given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name></person-group> (<year>2015</year>). <article-title>Subspecialization in the human posterior medial cortex</article-title>. <source>Neuroimage</source>, <volume>106</volume>, <fpage>55</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="pmid">25462801</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2014.11.009</pub-id><pub-id pub-id-type="pmcid">PMC4780672</pub-id></mixed-citation></ref><ref id="bib13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cohen Kadosh</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Henson</surname><given-names>R. N. A.</given-names></name>, <name name-style="western"><surname>Cohen Kadosh</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>M. H.</given-names></name>, &amp; <name name-style="western"><surname>Dick</surname><given-names>F.</given-names></name></person-group> (<year>2010</year>). <article-title>Task-dependent activation of face-sensitive cortex: An fMRI adaptation study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>22</volume>, <fpage>903</fpage>–<lpage>917</lpage>. <pub-id pub-id-type="pmid">19320549</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/jocn.2009.21224</pub-id></mixed-citation></ref><ref id="bib14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Collins</surname><given-names>D. L.</given-names></name>, <name name-style="western"><surname>Neelin</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Peters</surname><given-names>T. M.</given-names></name>, &amp; <name name-style="western"><surname>Evans</surname><given-names>A. C.</given-names></name></person-group> (<year>1994</year>). <article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title>. <source>Journal of Computer Assisted Tomography</source>, <volume>18</volume>, <fpage>192</fpage>–<lpage>205</lpage>.<pub-id pub-id-type="pmid">8126267</pub-id></mixed-citation></ref><ref id="bib15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cong</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Puoliväli</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Alluri</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Sipola</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Burunat</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Toiviainen</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Nandi</surname><given-names>A. K.</given-names></name>, <name name-style="western"><surname>Brattico</surname><given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Ristaniemi</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>). <article-title>Key issues in decomposing fMRI during naturalistic and continuous music experience with independent component analysis</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>223</volume>, <fpage>74</fpage>–<lpage>84</lpage>.<pub-id pub-id-type="pmid">24333752</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jneumeth.2013.11.025</pub-id></mixed-citation></ref><ref id="bib16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dick</surname><given-names>A. S.</given-names></name>, <name name-style="western"><surname>Mok</surname><given-names>E. H.</given-names></name>, <name name-style="western"><surname>Beharelle</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Goldin-Meadow</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2014</year>). <article-title>Frontal and temporal contributions to understanding the iconic co-speech gestures that accompany speech</article-title>. <source>Human Brain Mapping</source>, <volume>35</volume>, <fpage>900</fpage>–<lpage>917</lpage>.<pub-id pub-id-type="pmid">23238964</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.22222</pub-id><pub-id pub-id-type="pmcid">PMC3797208</pub-id></mixed-citation></ref><ref id="bib17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Bzdok</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Kurth</surname><given-names>F.</given-names></name>, &amp; <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name></person-group> (<year>2012</year>). <article-title>Activation likelihood estimation meta-analysis revisited</article-title>. <source>Neuroimage</source>, <volume>59</volume>, <fpage>2349</fpage>–<lpage>2361</lpage>. <pub-id pub-id-type="pmid">21963913</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2011.09.017</pub-id><pub-id pub-id-type="pmcid">PMC3254820</pub-id></mixed-citation></ref><ref id="bib18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name>, <name name-style="western"><surname>Bzdok</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Hensel</surname><given-names>L.</given-names></name></person-group> (<year>2016a</year>). <article-title>Functional segregation of the human dorsomedial prefrontal cortex</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>, <fpage>304</fpage>–<lpage>321</lpage>. <pub-id pub-id-type="pmid">25331597</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/cercor/bhu250</pub-id><pub-id pub-id-type="pmcid">PMC4677979</pub-id></mixed-citation></ref><ref id="bib19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Grefkes</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>L. E.</given-names></name>, <name name-style="western"><surname>Zilles</surname><given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name></person-group> (<year>2009</year>). <article-title>Coordinate-based activation likelihood estimation meta-analysis of neuroimaging data: A random-effects approach based on empirical estimates of spatial uncertainty</article-title>. <source>Human Brain Mapping</source>, <volume>30</volume>, <fpage>2907</fpage>–<lpage>2926</lpage>. <pub-id pub-id-type="pmid">19172646</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.20718</pub-id><pub-id pub-id-type="pmcid">PMC2872071</pub-id></mixed-citation></ref><ref id="bib20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Nichols</surname><given-names>T. E.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Hoffstaedter</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Amunts</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name>, <name name-style="western"><surname>Bzdok</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Eickhoff</surname><given-names>C. R.</given-names></name></person-group> (<year>2016b</year>). <article-title>Behavior, sensitivity, and power of activation likelihood estimation characterized by massive empirical simulation</article-title>. <source>Neuroimage</source>, <volume>137</volume>, <fpage>70</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="pmid">27179606</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2016.04.072</pub-id><pub-id pub-id-type="pmcid">PMC4981641</pub-id></mixed-citation></ref><ref id="bib21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Epstein</surname><given-names>R. A.</given-names></name></person-group> (<year>2008</year>). <article-title>Parahippocampal and retrosplenial contributions to human spatial navigation</article-title>. <source>Trends in Cognitive Sciences</source>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2008.07.004</pub-id><pub-id pub-id-type="pmcid">PMC2858632</pub-id><pub-id pub-id-type="pmid">18760955</pub-id></mixed-citation></ref><ref id="bib22"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Evans</surname><given-names>A. C.</given-names></name>, <name name-style="western"><surname>Collins</surname><given-names>D. L.</given-names></name>, <name name-style="western"><surname>Mills</surname><given-names>S. R.</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>E. D.</given-names></name>, <name name-style="western"><surname>Kelly</surname><given-names>R. L.</given-names></name>, &amp; <name name-style="western"><surname>Peters</surname><given-names>T. M.</given-names></name></person-group> (<year>1993</year>). <article-title>3D statistical neuroanatomical models from 305 MRI volumes</article-title>. In <source>1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference</source>, <publisher-name>IEEE</publisher-name>, pp. <fpage>1813</fpage>–<lpage>1817</lpage>. </mixed-citation></ref><ref id="bib23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fehr</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Achtziger</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Struber</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Neural correlates of the empathic perceptual processing of realistic social interaction scenarios displayed from a first-order perspective</article-title>. <source>Brain Research</source>, <volume>1583</volume>, <fpage>141</fpage>–<lpage>158</lpage>.<pub-id pub-id-type="pmid">24814646</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.brainres.2014.04.041</pub-id></mixed-citation></ref><ref id="bib24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>S. P.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>P. M.</given-names></name>, <name name-style="western"><surname>Uecker</surname><given-names>A. M.</given-names></name>, <name name-style="western"><surname>Crank</surname><given-names>M.</given-names></name>, … <name name-style="western"><surname>Lancaster</surname><given-names>J. L.</given-names></name></person-group> (<year>2005</year>). <article-title>Brainmap taxonomy of experimental design: Description and evaluation</article-title>. <source>Human Brain Mapping</source>, <volume>25</volume>, <fpage>185</fpage>–<lpage>198</lpage>. <pub-id pub-id-type="pmid">15846810</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.20141</pub-id><pub-id pub-id-type="pmcid">PMC6871758</pub-id></mixed-citation></ref><ref id="bib25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friederici</surname><given-names>A. D.</given-names></name>, &amp; <name name-style="western"><surname>Gierhan</surname><given-names>S. M.</given-names></name></person-group> (<year>2013</year>). <article-title>The language network</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume>, <fpage>250</fpage>–<lpage>254</lpage>. <pub-id pub-id-type="pmid">23146876</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.conb.2012.10.002</pub-id></mixed-citation></ref><ref id="bib26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K. J.</given-names></name>, <name name-style="western"><surname>Price</surname><given-names>C. J.</given-names></name>, <name name-style="western"><surname>Fletcher</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Moore</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Frackowiak</surname><given-names>R. S.</given-names></name>, &amp; <name name-style="western"><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>1996</year>). <article-title>The trouble with cognitive subtraction</article-title>. <source>Neuroimage</source>, <volume>4</volume>, <fpage>97</fpage>–<lpage>104</lpage>. <pub-id pub-id-type="pmid">9345501</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/nimg.1996.0033</pub-id></mixed-citation></ref><ref id="bib27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gazzola</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Keysers</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>The observation and execution of actions share motor and somatosensory voxels in all tested subjects:Single-subject analyses of unsmoothed fMRI data</article-title>. <source>Cerebral Cortex</source>, <volume>19</volume>, <fpage>1239</fpage>–<lpage>1255</lpage>. <pub-id pub-id-type="pmid">19020203</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/cercor/bhn181</pub-id><pub-id pub-id-type="pmcid">PMC2677653</pub-id></mixed-citation></ref><ref id="bib28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giard</surname><given-names>M. H.</given-names></name>, &amp; <name name-style="western"><surname>Peronnet</surname><given-names>F.</given-names></name></person-group> (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume>, <fpage>473</fpage>–<lpage>490</lpage>. <pub-id pub-id-type="pmid">10511637</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/089892999563544</pub-id></mixed-citation></ref><ref id="bib29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gorgolewski</surname><given-names>K. J.</given-names></name>, <name name-style="western"><surname>Varoquaux</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Rivera</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Schwarz</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Ghosh</surname><given-names>S. S.</given-names></name>, <name name-style="western"><surname>Maumet</surname><given-names>C.</given-names></name>, … <name name-style="western"><surname>Margulies</surname><given-names>D. S.</given-names></name> (</person-group><year>2015</year>). <article-title>NeuroVault.org: A web-based repository for collecting and sharing unthresholded statistical maps of the human brain</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>9</volume>, <fpage>8</fpage><pub-id pub-id-type="pmid">25914639</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fninf.2015.00008</pub-id><pub-id pub-id-type="pmcid">PMC4392315</pub-id></mixed-citation></ref><ref id="bib30"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gray</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Carver</surname><given-names>H. V.</given-names></name>, &amp; <name name-style="western"><surname>Standring</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <source>Gray’s Anatomy</source> (<edition>40th ed.</edition>). <publisher-name>Churchill Livingstone</publisher-name></mixed-citation></ref><ref id="bib31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hanke</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Adelhöfer</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Kottke</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Iacovella</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Sengupta</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kaule</surname><given-names>F. R.</given-names></name>, … <name name-style="western"><surname>Stadler</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation</article-title>. <source>Scientific Data</source>, <volume>3</volume>, <fpage>160092</fpage><pub-id pub-id-type="pmid">27779621</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/sdata.2016.92</pub-id><pub-id pub-id-type="pmcid">PMC5079121</pub-id></mixed-citation></ref><ref id="bib32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hanke</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Baumgartner</surname><given-names>F. J.</given-names></name>, <name name-style="western"><surname>Ibe</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Kaule</surname><given-names>F. R.</given-names></name>, <name name-style="western"><surname>Pollmann</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Speck</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Zinke</surname><given-names>W.</given-names></name>, &amp; <name name-style="western"><surname>Stadler</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie</article-title>. <source>Scientific Data</source>, <volume>1</volume><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/sdata.2014.3</pub-id><pub-id pub-id-type="pmcid">PMC4322572</pub-id><pub-id pub-id-type="pmid">25977761</pub-id></mixed-citation></ref><ref id="bib33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hanke</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Dinga</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Häusler</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Guntupalli</surname><given-names>J. S.</given-names></name>, <name name-style="western"><surname>Casey</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kaule</surname><given-names>F. R.</given-names></name>, &amp; <name name-style="western"><surname>Stadler</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>High-resolution 7-Tesla fMRI data on the perception of musical genres—An extension to the studyforrest dataset</article-title>. <source>F1000Research</source>. </mixed-citation></ref><ref id="bib34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hasson</surname><given-names>U.</given-names></name>, &amp; <name name-style="western"><surname>Honey</surname><given-names>C. J.</given-names></name></person-group> (<year>2012</year>). <article-title>Future trends in neuroimaging: Neural processes as expressed within real-life contexts</article-title>. <source>Neuroimage</source>, <volume>62</volume>, <fpage>1272</fpage>–<lpage>1278</lpage>. <pub-id pub-id-type="pmid">22348879</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2012.02.004</pub-id><pub-id pub-id-type="pmcid">PMC3360990</pub-id></mixed-citation></ref><ref id="bib35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heim</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Opitz</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Müller</surname><given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Friederici</surname><given-names>A. D.</given-names></name></person-group> (<year>2003</year>). <article-title>Phonological processing during language production: fMRI evidence for a shared production-comprehension network</article-title>. <source>Brain Research. Cognitive Brain Research</source>, <volume>16</volume>, <fpage>285</fpage>–<lpage>296</lpage>.<pub-id pub-id-type="pmid">12668238</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/s0926-6410(02)00284-7</pub-id></mixed-citation></ref><ref id="bib36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalpouzos</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Sjölie</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Molin</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Nyberg</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). <article-title>Neurocognitive systems related to real-world prospective memory</article-title>. <source>PLoS One</source>, <volume>5</volume>, <fpage>e13304</fpage><pub-id pub-id-type="pmid">20949046</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0013304</pub-id><pub-id pub-id-type="pmcid">PMC2951914</pub-id></mixed-citation></ref><ref id="bib37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kanungo</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Mount</surname><given-names>D. M.</given-names></name>, <name name-style="western"><surname>Netanyahu</surname><given-names>N. S.</given-names></name>, <name name-style="western"><surname>Piatko</surname><given-names>C. D.</given-names></name>, <name name-style="western"><surname>Silverman</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Wu</surname><given-names>A. Y.</given-names></name></person-group> (<year>2004</year>). <article-title>A local search approximation algorithm for k-means clustering</article-title>. <source>Computational Geometry</source>, <volume>28</volume>, <fpage>89</fpage>–<lpage>112</lpage>. </mixed-citation></ref><ref id="bib38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kauttonen</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Hlushchuk</surname><given-names>Y.</given-names></name>, &amp; <name name-style="western"><surname>Tikka</surname><given-names>P.</given-names></name></person-group> (<year>2015</year>). <article-title>Optimizing methods for linking cinematic features to fMRI data</article-title>. <source>Neuroimage</source>, <volume>110</volume>, <fpage>136</fpage>–<lpage>148</lpage>.<pub-id pub-id-type="pmid">25662868</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2015.01.063</pub-id></mixed-citation></ref><ref id="bib39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lahnakoski</surname><given-names>J. M.</given-names></name>, <name name-style="western"><surname>Glerean</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Salmi</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Jääskeläinen</surname><given-names>I. P.</given-names></name>, <name name-style="western"><surname>Sams</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hari</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Nummenmaa</surname><given-names>L.</given-names></name></person-group> (<year>2012</year>). <article-title>Naturalistic FMRI mapping reveals superior temporal sulcus as the hub for the distributed brain network for social perception</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>6</volume>, <fpage>233</fpage><comment>Available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22905026">http://www.ncbi.nlm.nih.gov/pubmed/22905026</ext-link> [Accessed October 24, 2017]</comment>.<pub-id pub-id-type="pmid">22905026</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fnhum.2012.00233</pub-id><pub-id pub-id-type="pmcid">PMC3417167</pub-id></mixed-citation></ref><ref id="bib40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>P. M.</given-names></name>, <name name-style="western"><surname>Price</surname><given-names>C. J.</given-names></name>, <name name-style="western"><surname>Glahn</surname><given-names>D. C.</given-names></name>, <name name-style="western"><surname>Uecker</surname><given-names>A. M.</given-names></name>, <name name-style="western"><surname>Lancaster</surname><given-names>J. L.</given-names></name>, … <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name></person-group> (<year>2005</year>). <article-title>ALE meta-analysis: Controlling the false discovery rate and performing statistical contrasts</article-title>. <source>Human Brain Mapping</source>, <volume>25</volume>, <fpage>155</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="pmid">15846811</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.20136</pub-id><pub-id pub-id-type="pmcid">PMC6871747</pub-id></mixed-citation></ref><ref id="bib41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Riedel</surname><given-names>M. C.</given-names></name>, <name name-style="western"><surname>Sutherland</surname><given-names>M. T.</given-names></name>, <name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Ray</surname><given-names>K. L.</given-names></name>, <name name-style="western"><surname>Uecker</surname><given-names>A. M.</given-names></name>, … <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name></person-group> (<year>2015</year>). <article-title>Neural architecture underlying classification of face perception paradigms</article-title>. <source>Neuroimage</source>, <volume>119</volume>, <fpage>70</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="pmid">26093327</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2015.06.044</pub-id><pub-id pub-id-type="pmcid">PMC4564321</pub-id></mixed-citation></ref><ref id="bib42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>J. L.</given-names></name>, <name name-style="western"><surname>McMillan</surname><given-names>K. M.</given-names></name>, <name name-style="western"><surname>Tordesillas-Gutiérrez</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>S. T.</given-names></name>, <name name-style="western"><surname>Gonzales</surname><given-names>S. M.</given-names></name>, … <name name-style="western"><surname>Lancaster</surname><given-names>J. L.</given-names></name></person-group> (<year>2010</year>). <article-title>Comparison of the disparity between Talairach and MNI coordinates in functional neuroimaging data: Validation of the Lancaster transform</article-title>. <source>Neuroimage</source>, <volume>51</volume>, <fpage>677</fpage>–<lpage>683</lpage>. <pub-id pub-id-type="pmid">20197097</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2010.02.048</pub-id><pub-id pub-id-type="pmcid">PMC2856713</pub-id></mixed-citation></ref><ref id="bib43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lancaster</surname><given-names>J. L.</given-names></name>, <name name-style="western"><surname>Tordesillas-Gutiérrez</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Martinez</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Salinas</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Evans</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Zilles</surname><given-names>K.</given-names></name>, … <name name-style="western"><surname>Fox</surname><given-names>P. T.</given-names></name></person-group> (<year>2007</year>). <article-title>Bias between MNI and Talairach coordinates analyzed using the ICBM-152 brain template</article-title>. <source>Human Brain Mapping</source>, <volume>28</volume>, <fpage>1194</fpage>–<lpage>1205</lpage>. <pub-id pub-id-type="pmid">17266101</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.20345</pub-id><pub-id pub-id-type="pmcid">PMC6871323</pub-id></mixed-citation></ref><ref id="bib44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>A. C. H.</given-names></name>, <name name-style="western"><surname>Buckley</surname><given-names>M. J.</given-names></name>, <name name-style="western"><surname>Pegman</surname><given-names>S. J.</given-names></name>, <name name-style="western"><surname>Spiers</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Scahill</surname><given-names>V. L.</given-names></name>, <name name-style="western"><surname>Gaffan</surname><given-names>D.</given-names></name>, … <name name-style="western"><surname>Graham</surname><given-names>K. S.</given-names></name></person-group> (<year>2005</year>). <article-title>Specialization in the medial temporal lobe for processing of objects and scenes</article-title>. <source>Hippocampus</source>, <volume>15</volume>, <fpage>782</fpage>–<lpage>797</lpage>. <pub-id pub-id-type="pmid">16010661</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hipo.20101</pub-id></mixed-citation></ref><ref id="bib45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name>, … <name name-style="western"><surname>Sun</surname><given-names>P.</given-names></name></person-group> (<year>2015</year>). <article-title>Crossmodal integration enhances neural representation of task-relevant features in audiovisual face perception</article-title>. <source>Cerebral Cortex</source>, <volume>25</volume>, <fpage>384</fpage>–<lpage>395</lpage>. <pub-id pub-id-type="pmid">23978654</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/cercor/bht228</pub-id></mixed-citation></ref><ref id="bib46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lindquist</surname><given-names>K. A.</given-names></name>, <name name-style="western"><surname>Wager</surname><given-names>T. D.</given-names></name>, <name name-style="western"><surname>Kober</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Bliss-Moreau</surname><given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Barrett</surname><given-names>L. F.</given-names></name></person-group> (<year>2012</year>). <article-title>The brain basis of emotion: A meta-analytic review</article-title>. <source>Behavioral and Brain Sciences</source>, (<issue>35</issue>), <fpage>121</fpage>–<lpage>143</lpage>. <pub-id pub-id-type="pmid">22617651</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1017/S0140525X11000446</pub-id><pub-id pub-id-type="pmcid">PMC4329228</pub-id></mixed-citation></ref><ref id="bib47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maguire</surname><given-names>E. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Studying the freely-behaving brain with fMRI</article-title>. <source>Neuroimage</source>, <volume>62</volume>, <fpage>1170</fpage>–<lpage>1176</lpage>. <pub-id pub-id-type="pmid">22245643</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2012.01.009</pub-id><pub-id pub-id-type="pmcid">PMC3480644</pub-id></mixed-citation></ref><ref id="bib48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marsh</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Hao</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Duan</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name>, … <name name-style="western"><surname>Peterson</surname><given-names>B. S.</given-names></name></person-group> (<year>2010</year>). <article-title>A virtual reality-based FMRI study of reward-based spatial learning</article-title>. <source>Neuropsychologia</source>, <volume>48</volume>, <fpage>2912</fpage>–<lpage>2921</lpage>. <pub-id pub-id-type="pmid">20570684</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuropsychologia.2010.05.033</pub-id><pub-id pub-id-type="pmcid">PMC2914178</pub-id></mixed-citation></ref><ref id="bib49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McGurk</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>MacDonald</surname><given-names>J.</given-names></name></person-group> (<year>1976</year>). <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>, <volume>264</volume>, <fpage>691</fpage>–<lpage>811</lpage>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/264746a0</pub-id><pub-id pub-id-type="pmid">1012311</pub-id></mixed-citation></ref><ref id="bib50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mišić</surname><given-names>B.</given-names></name>, &amp; <name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2016</year>). <article-title>From regions to connections and networks: New bridges between brain and behavior</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>40</volume>, <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="pmid">27209150</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.conb.2016.05.003</pub-id><pub-id pub-id-type="pmcid">PMC5056800</pub-id></mixed-citation></ref><ref id="bib51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moher</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Liberati</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Tetzlaff</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Altman</surname><given-names>D. G.</given-names></name>, &amp; <name name-style="western"><surname>Altman</surname><given-names>D.</given-names></name></person-group> (<year>2009</year>). <article-title>Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement</article-title>. <source>PLoS Medicine</source>, <volume>6</volume>, <fpage>e1000097</fpage><pub-id pub-id-type="pmid">19621072</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pmed.1000097</pub-id><pub-id pub-id-type="pmcid">PMC2707599</pub-id></mixed-citation></ref><ref id="bib52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Molenberghs</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Cunnington</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Mattingley</surname><given-names>J. B.</given-names></name></person-group> (<year>2009</year>). <article-title>Is the mirror neuron system involved in imitation? A short review and meta-analysis</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>33</volume>, <fpage>975</fpage>–<lpage>980</lpage>. <pub-id pub-id-type="pmid">19580913</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neubiorev.2009.03.010</pub-id></mixed-citation></ref><ref id="bib53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nardo</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Santangelo</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Macaluso</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>Spatial orienting in complex audiovisual environments</article-title>. <source>Human Brain Mapping</source>, <volume>35</volume>, <fpage>1597</fpage>–<lpage>1614</lpage>.<pub-id pub-id-type="pmid">23616340</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.22276</pub-id><pub-id pub-id-type="pmcid">PMC3786006</pub-id></mixed-citation></ref><ref id="bib54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pelphrey</surname><given-names>K. A.</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>J. P.</given-names></name>, &amp; <name name-style="western"><surname>McCarthy</surname><given-names>G.</given-names></name></person-group> (<year>2004</year>). <article-title>Grasping the intentions of others: The perceived intentionality of an action influences activity in the superior temporal sulcus during social perception</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>16</volume>, <fpage>1706</fpage>–<lpage>1716</lpage>. <pub-id pub-id-type="pmid">15701223</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/0898929042947900</pub-id></mixed-citation></ref><ref id="bib55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pelphrey</surname><given-names>K. A.</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>J. P.</given-names></name>, <name name-style="western"><surname>Michelich</surname><given-names>C. R.</given-names></name>, <name name-style="western"><surname>Allison</surname><given-names>T.</given-names></name>, &amp; <name name-style="western"><surname>McCarthy</surname><given-names>G.</given-names></name></person-group> (<year>2005</year>). <article-title>Functional anatomy of biological motion perception in posterior temporal cortex: An fMRI study of eye, mouth and hand movements</article-title>. <source>Cerebral Cortex</source>, <volume>15</volume>, <fpage>1866</fpage>–<lpage>1876</lpage>. <pub-id pub-id-type="pmid">15746001</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/cercor/bhi064</pub-id></mixed-citation></ref><ref id="bib56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Price</surname><given-names>C. J.</given-names></name></person-group> (<year>2010</year>). <article-title>The anatomy of language: A review of 100 fMRI studies published in 2009</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1191</volume>, <fpage>62</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="pmid">20392276</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1749-6632.2010.05444.x</pub-id></mixed-citation></ref><ref id="bib57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Puce</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Allison</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Bentin</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Gore</surname><given-names>J. C.</given-names></name>, &amp; <name name-style="western"><surname>McCarthy</surname><given-names>G.</given-names></name></person-group> (<year>1998</year>). <article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title>. <source>Journal of Neuroscience</source>, <volume>18</volume>, <fpage>2188</fpage>–<lpage>2199</lpage>.<pub-id pub-id-type="pmid">9482803</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1523/JNEUROSCI.18-06-02188.1998</pub-id><pub-id pub-id-type="pmcid">PMC6792917</pub-id></mixed-citation></ref><ref id="bib58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Puschmann</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Huster</surname><given-names>R. J.</given-names></name>, &amp; <name name-style="western"><surname>Thiel</surname><given-names>C. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Mapping the spatiotemporal dynamics of processing task-relevant and task-irrelevant sound feature changes using concurrent EEG-fMRI</article-title>. <source>Human Brain Mapping</source>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.23248</pub-id><pub-id pub-id-type="pmcid">PMC6867321</pub-id><pub-id pub-id-type="pmid">27280466</pub-id></mixed-citation></ref><ref id="bib59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reed</surname><given-names>C. L.</given-names></name>, <name name-style="western"><surname>Shoham</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Halgren</surname><given-names>E.</given-names></name></person-group> (<year>2004</year>). <article-title>Neural Substrates of Tactile Object Recognition: An fMRI Study</article-title>. <source>Human Brain Mapping</source>, <volume>21</volume>, <fpage>236</fpage>–<lpage>246</lpage>.<pub-id pub-id-type="pmid">15038005</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.10162</pub-id><pub-id pub-id-type="pmcid">PMC6871926</pub-id></mixed-citation></ref><ref id="bib60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sailer</surname><given-names>U.</given-names></name>, <name name-style="western"><surname>Eggert</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Ditterich</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Straube</surname><given-names>A.</given-names></name></person-group> (<year>2000</year>). <article-title>Spatial and temporal aspects of eye-hand coordination across different tasks</article-title>. <source>Experimental Brain Research</source>, <volume>134</volume>, <fpage>163</fpage>–<lpage>173</lpage>. <pub-id pub-id-type="pmid">11037283</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s002210000457</pub-id></mixed-citation></ref><ref id="bib61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saur</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Schelter</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Schnell</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Kratochvil</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Küpper</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kellmeyer</surname><given-names>P.</given-names></name>, … <name name-style="western"><surname>Weiller</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Combining functional and anatomical connectivity reveals brain networks for auditory language comprehension</article-title>. <source>Neuroimage</source>, <volume>49</volume>, <fpage>3187</fpage>–<lpage>3197</lpage>. <pub-id pub-id-type="pmid">19913624</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2009.11.009</pub-id></mixed-citation></ref><ref id="bib62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Crossmodal spatial attention</article-title>. <source>Annals of the New York Academy of Sciences</source>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1749-6632.2010.05440.x</pub-id><pub-id pub-id-type="pmid">20392281</pub-id></mixed-citation></ref><ref id="bib63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spiers</surname><given-names>H. J.</given-names></name>, &amp; <name name-style="western"><surname>Maguire</surname><given-names>E. A.</given-names></name></person-group> (<year>2006</year>). <article-title>Spontaneous mentalizing during an interactive real world task: An fMRI study</article-title>. <source>Neuropsychologia</source>, <volume>44</volume>, <fpage>1674</fpage>–<lpage>1682</lpage>. <pub-id pub-id-type="pmid">16687157</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuropsychologia.2006.03.028</pub-id></mixed-citation></ref><ref id="bib64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spreng</surname><given-names>R. N.</given-names></name>, <name name-style="western"><surname>Sepulcre</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Turner</surname><given-names>G. R.</given-names></name>, <name name-style="western"><surname>Stevens</surname><given-names>W. D.</given-names></name>, &amp; <name name-style="western"><surname>Schacter</surname><given-names>D. L.</given-names></name></person-group> (<year>2013</year>). <article-title>Intrinsic architecture underlying the relations among the default, dorsal attention, and frontoparietal control networks of the human brain</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>25</volume>, <fpage>74</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="pmid">22905821</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1162/jocn_a_00281</pub-id><pub-id pub-id-type="pmcid">PMC3816715</pub-id></mixed-citation></ref><ref id="bib65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spunt</surname><given-names>R. P.</given-names></name>, &amp; <name name-style="western"><surname>Lieberman</surname><given-names>M. D.</given-names></name></person-group> (<year>2012</year>). <article-title>An integrative model of the neural systems supporting the comprehension of observed emotional behavior</article-title>. <source>Neuroimage</source>, <volume>59</volume>, <fpage>3050</fpage>–<lpage>3059</lpage>. <pub-id pub-id-type="pmid">22019857</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2011.10.005</pub-id></mixed-citation></ref><ref id="bib66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Squire</surname><given-names>L. R.</given-names></name>, <name name-style="western"><surname>Stark</surname><given-names>C. E. L.</given-names></name>, &amp; <name name-style="western"><surname>Clark</surname><given-names>R. E.</given-names></name></person-group> (<year>2004</year>). <article-title>The medial temporal lobe</article-title>. <source>Annual Review of Neuroscience</source>, <volume>27</volume>, <fpage>279</fpage>–<lpage>306</lpage>. <pub-id pub-id-type="doi" assigning-authority="pmc">10.1146/annurev.neuro.27.070203.144130</pub-id><pub-id pub-id-type="pmid">15217334</pub-id></mixed-citation></ref><ref id="bib67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Summerfield</surname><given-names>J. J.</given-names></name>, <name name-style="western"><surname>Hassabis</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Maguire</surname><given-names>E. A.</given-names></name></person-group> (<year>2010</year>). <article-title>Differential engagement of brain regions within a “core” network during scene construction</article-title>. <source>Neuropsychologia</source>, <volume>48</volume>, <fpage>1501</fpage>–<lpage>1509</lpage>. <pub-id pub-id-type="pmid">20132831</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuropsychologia.2010.01.022</pub-id><pub-id pub-id-type="pmcid">PMC2850391</pub-id></mixed-citation></ref><ref id="bib68"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Talairach</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Tournoux</surname><given-names>P.</given-names></name></person-group> (<year>1988</year>). <source>Co-Planar Stereotaxic Atlas of the Human Brain: 3-D Proportional System: An Approach to Cerebral Imaging</source> (<edition>1st ed.</edition>). <publisher-loc>New York</publisher-loc>: <publisher-name>Thieme</publisher-name>.</mixed-citation></ref><ref id="bib69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Türe</surname><given-names>U.</given-names></name>, <name name-style="western"><surname>Yaşargil</surname><given-names>D. C. H.</given-names></name>, <name name-style="western"><surname>Al-Mefty</surname><given-names>O.</given-names></name>, &amp; <name name-style="western"><surname>Yaşargil</surname><given-names>M. G.</given-names></name></person-group> (<year>1999</year>). <article-title>Topographic anatomy of the insular region</article-title>. <source>Journal of Neurosurgery</source>, <volume>90</volume>, <fpage>720</fpage>–<lpage>733</lpage>. <pub-id pub-id-type="pmid">10193618</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3171/jns.1999.90.4.0720</pub-id></mixed-citation></ref><ref id="bib70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turkeltaub</surname><given-names>P. E.</given-names></name>, <name name-style="western"><surname>Eden</surname><given-names>G. F.</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>K. M.</given-names></name>, &amp; <name name-style="western"><surname>Zeffiro</surname><given-names>T. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Meta-analysis of the functional neuroanatomy of single-word reading: Method and validation</article-title>. <source>Neuroimage</source>, <volume>16</volume>, <fpage>765</fpage>–<lpage>780</lpage>. <pub-id pub-id-type="pmid">12169260</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/nimg.2002.1131</pub-id></mixed-citation></ref><ref id="bib71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turkeltaub</surname><given-names>P. E.</given-names></name>, <name name-style="western"><surname>Eickhoff</surname><given-names>S. B.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name>, <name name-style="western"><surname>Fox</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Wiener</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Fox</surname><given-names>P.</given-names></name></person-group> (<year>2012</year>). <article-title>Minimizing within-experiment and within-group effects in activation likelihood estimation meta-analyses</article-title>. <source>Human Brain Mapping</source>, <volume>33</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="pmid">21305667</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.21186</pub-id><pub-id pub-id-type="pmcid">PMC4791073</pub-id></mixed-citation></ref><ref id="bib72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Turner</surname><given-names>J. A.</given-names></name>, &amp; <name name-style="western"><surname>Laird</surname><given-names>A. R.</given-names></name></person-group> (<year>2012</year>). <article-title>The cognitive paradigm ontology: design and application</article-title>. <source>Neuroinformatics</source>, <volume>10</volume>, <fpage>57</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="pmid">21643732</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s12021-011-9126-x</pub-id><pub-id pub-id-type="pmcid">PMC3682219</pub-id></mixed-citation></ref><ref id="bib73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wallentin</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Nielsen</surname><given-names>A. H.</given-names></name>, <name name-style="western"><surname>Vuust</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Dohn</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Roepstorff</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Lund</surname><given-names>T. E.</given-names></name></person-group> (<year>2011</year>). <article-title>Amygdala and heart rate variability responses from listening to emotionally intense parts of a story</article-title>. <source>Neuroimage</source>, <volume>58</volume>, <fpage>963</fpage>–<lpage>973</lpage>. <pub-id pub-id-type="pmid">21749924</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2011.06.077</pub-id></mixed-citation></ref><ref id="bib74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Ren</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Nguyen</surname><given-names>V. T.</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Guo</surname><given-names>C. C.</given-names></name></person-group> (<year>2016</year>). <article-title>Test-retest reliability of functional connectivity networks during naturalistic fMRI paradigms</article-title>. <source>bioRxiv</source>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/hbm.23517</pub-id><pub-id pub-id-type="pmcid">PMC6867176</pub-id><pub-id pub-id-type="pmid">28094464</pub-id></mixed-citation></ref><ref id="bib75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wheaton</surname><given-names>K. J.</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>J. C.</given-names></name>, <name name-style="western"><surname>Syngeniotis</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>D. F.</given-names></name>, &amp; <name name-style="western"><surname>Puce</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Viewing the motion of human body parts activates different regions of premotor, temporal, and parietal cortex</article-title>. <source>Neuroimage</source>, <volume>22</volume>, <fpage>277</fpage>–<lpage>288</lpage>. <pub-id pub-id-type="pmid">15110018</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2003.12.043</pub-id></mixed-citation></ref><ref id="bib76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wolbers</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Weiller</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Büchel</surname><given-names>C.</given-names></name></person-group> (<year>2004</year>). <article-title>Neural foundations of emerging route knowledge in complex spatial environments</article-title>. <source>Cognitive Brain Research</source>, <volume>21</volume>, <fpage>401</fpage>–<lpage>411</lpage>. <pub-id pub-id-type="pmid">15511655</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cogbrainres.2004.06.013</pub-id></mixed-citation></ref><ref id="bib77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>C.-W.</given-names></name>, <name name-style="western"><surname>Krishnan</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Wager</surname><given-names>T. D.</given-names></name></person-group> (<year>2014</year>). <article-title>Cluster-extent based thresholding in fMRI analyses: Pitfalls and recommendations</article-title>. <source>Neuroimage</source>, <volume>91</volume>, <fpage>412</fpage>–<lpage>419</lpage>. <pub-id pub-id-type="pmid">24412399</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2013.12.058</pub-id><pub-id pub-id-type="pmcid">PMC4214144</pub-id></mixed-citation></ref><ref id="bib78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Kemeny</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Park</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Frattali</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Braun</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>Language in context: Emergent features of word, sentence, and narrative comprehension</article-title>. <source>Neuroimage</source>, <volume>25</volume>, <fpage>1002</fpage>–<lpage>1015</lpage>. <pub-id pub-id-type="pmid">15809000</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2004.12.013</pub-id></mixed-citation></ref><ref id="bib79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yarkoni</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Poldrack</surname><given-names>R. A.</given-names></name>, <name name-style="western"><surname>Nichols</surname><given-names>T. E.</given-names></name>, <name name-style="western"><surname>Van Essen</surname><given-names>D. C.</given-names></name>, &amp; <name name-style="western"><surname>Wager</surname><given-names>T. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature Methods</source>, <volume>8</volume>, <fpage>665</fpage>–<lpage>670</lpage>. <pub-id pub-id-type="pmid">21706013</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/nmeth.1635</pub-id><pub-id pub-id-type="pmcid">PMC3146590</pub-id></mixed-citation></ref></ref-list></back></article>